{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 663 Final Project: “The No-U-Turn Sampler”  \n",
    "\n",
    "# Sarah Normoyle, Gonzalo Bustos  \n",
    "# April 27, 2016  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. Introduction\n",
    "2. Background\n",
    "3. Implementation  \n",
    "        a. Unoptimized Code\n",
    "        b. Optimized Code\n",
    "        c. Speed Up Ratios\n",
    "4. Examples/Testing\n",
    "        a. Poisson\n",
    "        b. 5 dimensional MVN\n",
    "        c. 10 dimensional MVN\n",
    "5. Comparison\n",
    "6. Conclusion\n",
    "7. References \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many models, Monte Carlo Markov Chain (MCMC) methods such as Gibbs sampling and the Metropolis Hasting algorithm may not be efficient and may require a long time to converge. By using steps that are evaluated from the first-order gradient of the log posterior, Hamiltonian Monte Carlo (HMC) is an efficient MCMC algorithm that does not use random walk behavior. This paper by Matthew D. Hoffman and Andrew Gelman introduces a new algorithm, called the No-U-Turn Sampler (NUTS) that is an extension of Hamiltonian Monte Carlo. Unlike HMC, NUTS does not require the specification of the parameter for the number of steps, L. In addition, the use of a dual averaging technique is extended from HMC to NUTS in order to avoid the specification of a step size parameter, $\\epsilon$. Therefore, unlike HMC, NUTS can be implemented without having to hand-tune both of the two parameters, L and $\\epsilon$. In our report, we will implement the Naive NUTS Algorithm and also extend to the NUTS Algorithm with Dual Averaging. We will compare the efficiency and the results of this algorithm to other MCMC algorithms in Stan when used for a specific model and data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Hamiltonian Monte Carlo (HMC) depends strongly on choosing suitable values for $\\varepsilon$ and $L$, which is the number of times chosen to run the leapfrog step. If $\\varepsilon$ is too large, then the simulation will be inaccurate and yield low acceptance rates. If $\\varepsilon$ is too small, then computation will be wasted taking many small steps. If $L$ is too small, then successive samples will be close to one another, resulting in undesirable random walk behavior and slow mixing. If $L$ is too large, then HMC will generate trajectories that loop back and retrace their steps.\n",
    "\n",
    "The No-U-Turn Sampler (NUTS) is an extension of HMC that eliminates the need to specify a fixed value of $L$, the number of leapfrog steps. It also incorporates schemes for setting $\\varepsilon$ based on a dual averaging algorithm.\n",
    "\n",
    "Algorithm 1 from the paper goes through the Leapfrog function, which is also used in HMC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leapfrog step** from **Algorithm 1**\n",
    "\n",
    "**function** Leapfrog$(\\theta,r,\\varepsilon)$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "Set $\\tilde{\\theta} \\leftarrow \\theta + \\varepsilon \\tilde{r}$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\tilde{\\theta})$\n",
    "\n",
    "**return** $\\tilde{\\theta},\\tilde{r}$\n",
    "\n",
    "$\\mathcal{L}$ is the logarithm of the joint density of the variables of interest $\\theta$. The Leapfrog function of Algorithm 1 implements the Stormer-Verlet (\"leapfrog\") integrator, which proceeds according to the updates:\n",
    "\n",
    "$r^{t + \\frac{\\varepsilon}{2}} = r^t + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^t)$\n",
    "\n",
    "$\\theta^{t + \\varepsilon} = \\theta^t + \\varepsilon r^{t + \\frac{\\varepsilon}{2}}$\n",
    "\n",
    "$r^{t + \\varepsilon} = r^{t + \\frac{\\varepsilon}{2}} + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^{t + \\varepsilon})$\n",
    "\n",
    "where $r^t$ and $\\theta^t$ denote the values of the momentum and position variables $r$ and $\\theta$ at time $t$, $\\nabla_\\theta$ denotes the gradient with respect to $\\theta$ and $\\varepsilon$ is the step size parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing Algorithm 3, Efficient NUTS, the paper develops Algorithm 2, Naive NUTS. Algorithm 2 introduces a slice variable $u$ with conditional distribution $p(u|\\theta,r)=\\text{Uniform}(u;[0,\\text{exp}\\{\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\}])$, which renders the conditional distribution $p(\\theta,r|u) = \\text{Uniform} (\\theta,r;\\{\\theta',r'|\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\} \\geq u \\})$. After resampling $u|\\theta,r$, NUTS uses the leapfrog algorithm to trace out a path forwards and backwards, doing for 1 step, 2 steps, 4 steps, etc. This doubling process builds a balanced binary tree whose leaf-nodes correspond to position-momentum states. The process is halted when the trajectory starts to double back on itself. \n",
    "\n",
    "In summary, Algorithm 2 leaves the target distribution $p(\\theta) \\propto \\text{exp}\\{\\mathcal{L}(\\theta)\\}$ invariant. It achieves this by resampling the momentum and slice variables $r$ and $u$, simulating a Hamiltonian trajectory forwards and backwards in time until that trajectory either begins retracing its steps or encounters a state with very low probability, selecting a subset of the states encountered on that trajectory that lie within the slice defined by the slice variable $u$, and finally choosing the next position and momentum variables $\\theta^m$ and $r$ uniformly at random from the subset of the states encountered.\n",
    "\n",
    "Algorithm 3 improves Algorithm 2 by breaking out of the recursion as soon as a zero value for the stop indicator $s$ is encountered.\n",
    "\n",
    "Below is the first implementation of the NUTS sampler. It uses the Leapfrog function used in Hamiltonian Monte Carlo as well as a new BuildTree function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 3** Efficient NUTS\n",
    "\n",
    "Given $\\theta^0, \\varepsilon, \\mathcal{L}, M$:\n",
    "\n",
    "**for** $m=1$ to $M$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $r^0 \\sim \\mathcal{N}(0,I)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $u \\sim \\text{Uniform}([0, \\text{exp}\\{\\mathcal{L}(\\theta^{m-1}) - \\frac{1}{2} r^0 \\cdot r^0 \\}])$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize $\\theta^- = \\theta^{m-1},~\\theta^+ = \\theta^{m-1},~r^- = r^0,~r^+ = r^0,~j = 0,~\\theta^m=\\theta^{m-1},~n=1,~s=1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **while** $s=1$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose a direction $v_j \\sim \\text{Uniform}(\\{-1,1\\})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v_j=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\text{min}\\big\\{1,\\frac{n'}{n}\\big\\}$, set $\\theta^m \\leftarrow \\theta'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n \\leftarrow n + n'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s \\leftarrow s' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j \\leftarrow j+1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
    "\n",
    "**end for**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**function** BuildTree$(\\theta,r,u,v,j,\\varepsilon)$\n",
    "\n",
    "**if** $j=0$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Base case - take one leapfrog step in the direction $v$*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta',r' \\leftarrow \\text{Leapfrog}(\\theta,r,v\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow \\mathbb{1}[u \\leq \\text{exp}\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' \\}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow \\mathbb{1}[\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' > \\text{log}~u - \\Delta_{max}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta',r',\\theta',r',\\theta',n',s'$\n",
    "\n",
    "**else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Recursion - implicitly build the left and right subtrees*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta,r,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\frac{n''}{n'+n''}$, set $\\theta' \\leftarrow \\theta''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow s'' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow n' + n''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s'$\n",
    "\n",
    "**end if**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 6 further improves NUTS by adaptively tuning $\\varepsilon$. In order to do this the paper uses the stichastic approximation method, using a statistic $H_t$ that describes some aspect of the behavior of an MCMC algorithm at iteration $t \\geq 1$. The expectation of $H_t$ is defined as:\n",
    "\n",
    "$h(x) = \\mathbb{E}[H_t|x] = \\lim_{T\\to\\infty}\\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}[H_t|x]$\n",
    "\n",
    "$x \\in \\mathbb{R}$ is a tunable parameter to the MCMC algorithm. If $h$ is a non-decreasing function of $x$, the update $x_{t+1} \\leftarrow x_t - \\eta_t H_t$ is guaranteed to cause $h(x_t)$ to converge to 0, as long as the step size $\\eta_t$ satisfies the following:\n",
    "\n",
    "$\\sum_t \\eta_t = \\infty$\n",
    "\n",
    "$\\sum_t \\eta^2_t < \\infty$\n",
    "\n",
    "These conditions are satisfied for $\\eta_t = t^{-\\kappa}$, for $\\kappa \\in (0.5,1]$.\n",
    "\n",
    "Adapting the dual averaging scheme, an algorithm for nonsmooth and stochastic convex optimization, to the problem of MCMC adaptation by replacing stochastic gradients with the statistic $H_t$, the updates are the following:\n",
    "\n",
    "(i)$~~x_{t+1} \\leftarrow \\mu - \\frac{\\sqrt{t}}{\\gamma} \\frac{1}{t+t_0} \\sum_{i=1}^t H_i$\n",
    "\n",
    "(ii)$~~\\bar{x}_{t+1} \\leftarrow \\eta_t x_{t+1} + (1 - \\eta_t) \\bar{x}_t$\n",
    "\n",
    "Where $\\mu$ is a chosen point to where the iterates $x_t$ are shrunk towards, $\\gamma > 0$ is a parameter that controls the amount of shrinkage towards $\\mu$, $t_0 \\geq 0$ is a parameter that stabilizes the initial iterations and $\\bar{x}_1 = x_1$. The paper uses te values $\\gamma=0.05$, $t_0=10$ and $\\kappa=0.75$, obtained by trying settings by hand on a model used as an example.\n",
    "\n",
    "Eventhough, the dual averaging scheme should work for any initial value $\\varepsilon_1$ and any shrinkage target $\\mu$, the paper recommends choosing an initial value $\\varepsilon_1$ according to the heuristic in Algorithm 4, which results in a value that is small enough to produce reasonably accurate simulations but large enough to avoid wasting large amounts of computation. The paper also recommends setting $\\mu=log(10\\varepsilon_1)$, in order to use larger values of $\\varepsilon$ and save computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 4** Heuristic for choosing an initial values of $\\varepsilon$\n",
    "\n",
    "**function** FindReasonableEpsilon($\\theta$)\n",
    "\n",
    "Initialize $\\varepsilon =1, r \\sim \\mathcal{N}(0,I)$  \n",
    "\n",
    "Set $\\theta', r' \\leftarrow \\text{Leapfrog}(\\theta,r,\\varepsilon)$\n",
    "\n",
    "$a \\leftarrow 2 \\mathbb{1} \\Big[\\frac{p(\\theta',r')}{p(\\theta,r)} > 0.5\\Big]  - 1$\n",
    "\n",
    "**while** $\\Big(\\frac{p(\\theta',r')}{p(\\theta,r)}\\Big)^a > 2^{-a}$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\varepsilon \\leftarrow 2^a \\varepsilon$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\theta', r' \\leftarrow \\text{Leapfrog}(\\theta,r,\\varepsilon)$\n",
    "\n",
    "**end while**\n",
    "\n",
    "**return** $\\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the dual averaging scheme for $\\varepsilon$ it is necessary to define, for each iteration, the following statistic:\n",
    "\n",
    "$H_t^{NUTS} = \\frac{1}{|\\mathcal{B}_t^{final}|} \\sum_{\\theta,r \\in \\mathcal{B}_t^{final}} \\text{min}\\Big\\{1,\\frac{p(\\theta,r)}{p(\\theta^{t-1},r^{t,0})} \\Big\\}$\n",
    "\n",
    "$h^{NUTS} = \\mathbb{E} [H_t^{NUTS}]$\n",
    "\n",
    "$\\mathcal{B}_t^{final}$ is the set of all the states explored during the final iteration $t$ of the Markov chain and $\\theta^{t-1}$ and $r^{t,0}$ are the initial position and momentum for the $t^{th}$ iteration of the Markov chain. Assuming that $H^{NUTS}$ is non-increasing in $\\varepsilon$, equations (i) and (ii) can be used with $H_t = \\delta - H^{NUTS}$ and $x = \\text{log}~\\varepsilon$ to coerce $h^{NUTS} = \\delta$ for any $\\delta \\in (0,1)$.\n",
    "\n",
    "Algorithm 6 implements NUTS while incorporating the dual averaging algorithm, requiring a target mean acceptance probability $\\delta$ and a number of iterations $M^{adapt}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 6** NUTS with Dual Averaging\n",
    "\n",
    "Given $\\theta^0, \\delta, \\mathcal{L}, M, M^{adapt}$:\n",
    "\n",
    "Set $\\varepsilon_0=\\text{FindReasonableEpsilon}(\\theta), \\mu=\\text{log}(10\\varepsilon_0), \\bar{\\varepsilon}_0=1, \\bar{H}=0, \\gamma=0.05, t_0=10,\\kappa=0.75$\n",
    "\n",
    "**for** $m=1$ to $M$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Sample $r^0 \\sim \\mathcal{N}(0,I)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $u \\sim \\text{Uniform}([0, \\text{exp}\\{\\mathcal{L}(\\theta^{m-1}) - \\frac{1}{2} r^0 \\cdot r^0 \\}])$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize $\\theta^- = \\theta^{m-1},~\\theta^+ = \\theta^{m-1},~r^- = r^0,~r^+ = r^0,~j = 0,~\\theta^m=\\theta^{m-1},~n=1,~s=1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **while** $s=1$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose a direction $v_j \\sim \\text{Uniform}(\\{-1,1\\})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v_j=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta',n',s',\\alpha,n_\\alpha \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v_j,j,\\varepsilon_{m-1},\\theta^{m-1},r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta',n',s',\\alpha,n_\\alpha \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v_j,j,\\varepsilon_{m-1},\\theta^{m-1},r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\text{min}\\big\\{1,\\frac{n'}{n}\\big\\}$, set $\\theta^m \\leftarrow \\theta'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n \\leftarrow n + n'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s \\leftarrow s' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j \\leftarrow j+1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $m \\leq M^{adapt}$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\bar{H}_m = \\Big(1 - \\frac{1}{m+t_0} \\Big) \\bar{H}_{m-1} + \\frac{1}{m+t_0} \\Big( \\delta + - \\frac{\\alpha}{n_\\alpha} \\Big)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\text{log}~\\varepsilon_m = \\mu - \\frac{\\sqrt{m}}{\\gamma} \\bar{H}_m, \\text{log}~\\bar{\\epsilon}_m = m^{-\\kappa}~\\text{log}~\\varepsilon_m + (1 - m^{-\\kappa})~\\text{log}~\\bar{\\varepsilon}_{m-1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\varepsilon_m = \\bar{\\varepsilon}_{M^{adapt}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "**end for**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**function** BuildTree$(\\theta,r,u,v,j,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "**if** $j=0$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Base case - take one leapfrog step in the direction $v$*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta',r' \\leftarrow \\text{Leapfrog}(\\theta,r,v\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow \\mathbb{1}[u \\leq \\text{exp}\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' \\}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow \\mathbb{1}[\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' > \\text{log}~u - \\Delta_{max}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta',r',\\theta',r',\\theta',n',s',\\text{min}\\{1,exp\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' - \\mathcal{L}(\\theta^0) + \\frac{1}{2} r^0 \\cdot r^0 \\} \\}, 1$ \n",
    "\n",
    "**else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Recursion - implicitly build the left and right subtrees*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s',\\alpha',n'_\\alpha \\leftarrow \\text{BuildTree}(\\theta,r,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta'',n'',s'',\\alpha'',n''_\\alpha \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta'',n'',s'',\\alpha'',n''_\\alpha \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\frac{n''}{n'+n''}$, set $\\theta' \\leftarrow \\theta''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow s'' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow n' + n''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s',\\alpha',n'_\\alpha$\n",
    "\n",
    "**end if**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to implement Algorithm 6, the NUTS sampler with Dual Averaging adapted from Hamiltonian Monte Carlo because it does not require the specification of a value of epsilon. This algorithm requires 3 additional helper functions, FindReasonableEpsilon, BuildTree, and Leapfrog. Therefore, in total we implemented 4 algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first implementation, we followed the pseudocode pretty much exactly as follows above. This requires re-calculating the gradient many times in each of the helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Leapfrog_slow(theta, r, eps, L):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    From Algorithm 1 in NUTS, Leapfrog step in Hamiltonian Monte Carlo/NUTS.\n",
    "    Helper function for Algorithm 6. \n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "       \n",
    "    OUTPUTS:\n",
    "    theta_tilde = update theta vector\n",
    "    \n",
    "    r_tilde = updated r momenta vector\n",
    "    \"\"\"\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    r_tilde = r + (eps/2) * grad\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    logp_tilde, grad_tilde = L(theta_tilde)\n",
    "    r_tilde = r_tilde + (eps/2) * grad_tilde\n",
    "    return theta_tilde, r_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindReasonableEpsilon_slow(theta, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    From Algorithm 4 in NUTS.\n",
    "    Heuristic for choosing an initial value of epsilon.\n",
    "    Helper function for Algorithm 6.\n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    eps = value for initial value of epsilon, step size\n",
    "    \n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    parems = len(theta)\n",
    "    eps = 1\n",
    "    r = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "    r = r.ravel()\n",
    "    theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L)\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    logp_prime, grad_prime = L(theta_prime)\n",
    "    \n",
    "    prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r, r)))\n",
    "    a = 2 * int(prob > 0.5) - 1\n",
    "\n",
    "    while prob**a > 2**(-a):\n",
    "        eps = 2**a * eps\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L);\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r,r)))\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree_slow(theta, r, u, v, j, eps, r_theta0, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    builds tree in NUTS sampler, helper function for Algorithm 6\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    v = direction in creating tree, value from -1 to 1\n",
    "    \n",
    "    j = height of tree, starts at 0\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    r_theta0 = joint probability of theta0 and r\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime\n",
    "    \n",
    "    \"\"\"\n",
    "    if j == 0: \n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, v*eps, L)\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        \n",
    "        r_theta = logp_prime - 0.5 * np.dot(r_prime, r_prime)      \n",
    "\n",
    "        n_prime = int(u <= np.exp(r_theta))   \n",
    "        s_prime = int(r_theta > np.log(u) - 1000)\n",
    "        alpha_prime = min(1, np.exp(r_theta - r_theta0))\n",
    "                                                   \n",
    "        return theta_prime, r_prime, theta_prime, r_prime, theta_prime, n_prime, s_prime, alpha_prime, 1\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = BuildTree_slow(theta, r, u, v, j-1, eps, r_theta0, L)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, _,_, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_minus, r_minus, u, v, j-1, eps, r_theta0, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_plus, r_plus, u, v, j-1, eps, r_theta0, L)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / max(n_prime + n_doub_prime,1)\n",
    "            if (np.random.uniform(0, 1, 1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_prime * s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "            alpha_prime = alpha_prime + alpha_doub_prime\n",
    "            n_alpha_prime = n_alpha_prime + n_alpha_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nuts6_dual_averaging_slow(theta0, M, M_adapt, L, delta = 0.6):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    Not optimized version. \n",
    "    Implemented from Algorithm 6: NUTS with Dual Averaging.\n",
    "    Needs helper functions BuildTree_slow, FindReasonableEpsilon_slow, and Leapfrog_slow.\n",
    "    \n",
    "\n",
    "    INPUTS:\n",
    "    \n",
    "    theta0 = initial values for values of parameters in model. len(theta0) = number of parameters\n",
    "    \n",
    "    M = number of samples desired\n",
    "    \n",
    "    M_adapt = the number of steps for the burn-in,\n",
    "    also how long to run the dual averaging algorithm to find the appropriate epsilon\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    delta = target HMC acceptance probability.\n",
    "    default value of 0.6\n",
    "    is a value between 0 and 1\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    samples = np.array matrix of samples of theta from algorithm\n",
    "    dimensions of matrix are M x len(theta0)\n",
    "    \n",
    "    burned_in = same as samples matrix with burn_in samples removed\n",
    "    dimensions of matrix are are M-M_adapt x len(theta0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    parems = len(theta0)\n",
    "    samples = np.empty((M+1, parems))\n",
    "    samples[0, :] = theta0\n",
    "    eps = FindReasonableEpsilon_slow(theta0, L)\n",
    "    mu = np.log(10*eps)\n",
    "    eps_bar = 1\n",
    "    H_bar = 0\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    k = 0.75\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "        logp, grad = L(samples[m-1,:])\n",
    "\n",
    "        r_theta = logp - 0.5 * np.dot(r0,r0)\n",
    "        # resample u ~ uniform([0, exp(inside)])\n",
    "        u = np.random.uniform(0, np.exp(r_theta), 1)\n",
    "\n",
    "        # initialize minus's and plus's\n",
    "        theta_minus = samples[m-1, :]\n",
    "        theta_plus = samples[m-1, :]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0 \n",
    "        \n",
    "        samples[m, :] = samples[m-1, :]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, _, _, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_minus, r_minus, u, v_j, j, eps, r_theta, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_plus, r_plus, u, v_j, j, eps, r_theta, L)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m, :] = theta_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "            \n",
    "        if m <= M_adapt:\n",
    "            H_bar = (1 - 1/(m+t0))*H_bar + (1/(m+t0)) * (delta - alpha/n_alpha)\n",
    "            eps = np.exp(mu - np.sqrt(m)/gamma * H_bar)\n",
    "            eps_bar = np.exp(m**(-k) * np.log(eps) + (1-m**(-k))*np.log(eps_bar))\n",
    "        else:\n",
    "            eps = eps_bar\n",
    "            \n",
    "    burned_in = samples[M_adapt+1:M+1, :]\n",
    "    \n",
    "    return samples, burned_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the code, we first tried jit from Numba to speed up the code. There was no speed ups determined, and we considered other options. \n",
    "\n",
    "Next, we made sure opportunities for vectorization were implemented, which they were in places that were needed. \n",
    "\n",
    "Next, we noticed that the log likelihood and the gradient had to be re-calculated for values of $\\theta$ and $\\theta'$ when being passed through various functions, such as Leapfrog and BuildTree. The more dimensions of $\\theta$ we have, the longer re-calculating these values takes. Therfore, we can add the values of the log likelihood and the gradient as outputs and inputs for Leapfrom, BuildTree, and FindReasonableEpsilon. If the values of the log likelihood and gradient follows the value of $\\theta$ throughout the algorithm, then we thought algorithm would speed up. We thus implemented new functions for each of the four functions. These functions ended up being faster. Their relative speeds is investigated with specific examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Leapfrog(theta, r, grad, eps, L):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    From Algorithm 1 in NUTS, Leapfrog step in Hamiltonian Monte Carlo/NUTS.\n",
    "    Helper function for Algorithm 6. \n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "       \n",
    "    OUTPUTS:\n",
    "    theta_tilde = update theta vector\n",
    "    \n",
    "    r_tilde = updated r momenta vector\n",
    "    \"\"\"\n",
    "    \n",
    "    r_tilde = r + (eps/2) * grad\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    logp_tilde, grad_tilde = L(theta_tilde)\n",
    "    r_tilde = r_tilde + (eps/2) * grad_tilde\n",
    "    return theta_tilde, r_tilde, grad_tilde, logp_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindReasonableEpsilon(theta, grad, logp, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    From Algorithm 4 in NUTS.\n",
    "    Heuristic for choosing an initial value of epsilon.\n",
    "    Helper function for Algorithm 6.\n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    eps = value for initial value of epsilon, step size\n",
    "    \n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    parems = len(theta)\n",
    "    eps = 1\n",
    "    r = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "    r = r.ravel()\n",
    "    theta_prime, r_prime, _, logp_prime = Leapfrog(theta, r, grad, eps, L)\n",
    "    \n",
    "    prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r, r)))\n",
    "    a = 2 * int(prob > 0.5) - 1\n",
    "\n",
    "    while prob**a > 2**(-a):\n",
    "        eps = 2**a * eps\n",
    "        theta_prime, r_prime, _, logp_prime = Leapfrog(theta, r, grad, eps, L);\n",
    "        prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r,r)))\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree(theta, r, grad, u, v, j, eps, L, esto0):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    builds tree in NUTS sampler, helper function for Algorithm 6\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    v = direction in creating tree, value from -1 to 1\n",
    "    \n",
    "    j = height of tree, starts at 0\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    r_theta0 = joint probability of theta0 and r\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if j == 0: \n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime, grad_prime, logp_prime = Leapfrog(theta, r, grad, v*eps, L)\n",
    "        \n",
    "        esto = logp_prime - 0.5 * np.dot(r_prime, r_prime)\n",
    "\n",
    "        n_prime = int(u <= np.exp(esto))\n",
    "        \n",
    "        s_prime = int(esto > np.log(u) - 1000)\n",
    "\n",
    "        alpha_prime = min(1, np.exp(esto - esto0))\n",
    "                                                   \n",
    "        return theta_prime, r_prime, grad_prime, theta_prime, r_prime, grad_prime, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, 1\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, grad_minus, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = BuildTree(theta, r, grad, u, v, j-1, eps, L, esto0)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, grad_minus, _,_,_, theta_doub_prime, grad_doub_prime, logp_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree(theta_minus, r_minus, grad_minus, u, v, j-1, eps, L, esto0)\n",
    "            else:\n",
    "                _, _, _, theta_plus, r_plus, grad_plus, theta_doub_prime, grad_doub_prime, logp_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree(theta_plus, r_plus, grad_plus, u, v, j-1, eps, L, esto0)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / max(n_prime + n_doub_prime,1)\n",
    "            if (np.random.uniform(0, 1, 1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "                grad_prime = grad_doub_prime\n",
    "                logp_prime = logp_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_prime * s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "            alpha_prime = alpha_prime + alpha_doub_prime\n",
    "            n_alpha_prime = n_alpha_prime + n_alpha_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, grad_minus, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, n_alpha_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nuts6_dual_averaging(theta0, M, M_adapt, L, delta = 0.6):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    Implemented from Algorithm 6: NUTS with Dual Averaging.\n",
    "    Needs helper functions BuildTree, FindReasonableEpsilon, and Leapfrog.\n",
    "    \n",
    "\n",
    "    INPUTS:\n",
    "    \n",
    "    theta0 = initial values for values of parameters in model. len(theta0) = number of parameters\n",
    "    \n",
    "    M = number of samples desired\n",
    "    \n",
    "    M_adapt = the number of steps for the burn-in,\n",
    "    also how long to run the dual averaging algorithm to find the appropriate epsilon\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    delta = target HMC acceptance probability.\n",
    "    default value of 0.6\n",
    "    is a value between 0 and 1\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    samples = np.array matrix of samples of theta from algorithm\n",
    "    dimensions of matrix are M x len(theta0)\n",
    "    \n",
    "    burned_in = same as samples matrix with burn_in samples removed\n",
    "    dimensions of matrix are are M-M_adapt x len(theta0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    parems = len(theta0)\n",
    "    samples = np.empty((M+1, parems))\n",
    "    samples[0, :] = theta0\n",
    "    logp, grad = L(theta0)\n",
    "    eps = FindReasonableEpsilon(theta0, grad, logp, L)\n",
    "    mu = np.log(10*eps)\n",
    "    eps_bar = 1\n",
    "    H_bar = 0\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    k = 0.75\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "\n",
    "        esto = logp - 0.5 * np.dot(r0,r0)\n",
    "        # resample u ~ uniform([0, exp(inside)])\n",
    "        u = np.random.uniform(0, np.exp(esto), 1)\n",
    "\n",
    "        # initialize minus's and plus's\n",
    "        theta_minus = samples[m-1, :]\n",
    "        theta_plus = samples[m-1, :]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0\n",
    "        grad_minus = grad\n",
    "        grad_plus = grad\n",
    "        \n",
    "        j = 0\n",
    "        samples[m, :] = samples[m-1, :]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, grad_minus, _, _, _, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha, n_alpha = BuildTree(theta_minus, r_minus, grad_minus, u, v_j, j, eps, L, esto)\n",
    "            else:\n",
    "                _, _, _, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha, n_alpha = BuildTree(theta_plus, r_plus, grad_plus, u, v_j, j, eps, L, esto)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m, :] = theta_prime\n",
    "                    logp = logp_prime\n",
    "                    grad = grad_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "        \n",
    "        eta = 1 / (m + t0)\n",
    "        H_bar = (1 - eta)*H_bar + eta * (delta - alpha/n_alpha) \n",
    "        if m <= M_adapt:  \n",
    "            eps = np.exp(mu - np.sqrt(m)/gamma * H_bar)\n",
    "            eta = m**(-k)\n",
    "            esp_bar = np.exp((1-eta) * np.log(eps_bar) + eta * np.log(eps))\n",
    "        else:\n",
    "            eps = eps_bar\n",
    "            \n",
    "    burned_in = samples[M_adapt+1:M+1, :]\n",
    "    return samples, burned_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running examples (seen more in depth below), we calculated the speed up ratios for various examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of examples of speed up ratios is shown below. The examples are further explained below. \n",
    "\n",
    "With 5000 iterations of the algorithm and a data set of 100 observations, the times and the speed up ratios are shown below. The speed up ratios get higher as the model gets more complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Slow         | Fast     | Speed-Up Ratio |\n",
    "|------------|--------------|----------|----------------|\n",
    "| Poisson    | 1.5 sec      | 1.04 sec | 1.44           |\n",
    "| MVN (5-d)  | 2 min 39 sec | 34.5 sec | 4.61           |\n",
    "| MVN (10-d) | 2 min 44 sec | 32.7 sec | 5.02           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Poisson Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tested the code with a simple Poisson example. We generated simulated data from a Poisson distribution, and then fit a Poisson model likelihood with an improper uniform prior. We ran the slow and the fast alogithm. The mean of the sampled $\\theta's$ was very close to the mean of the data, so we considered this successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate random poisson data\n",
    "X_pois = np.random.poisson(5, 100)\n",
    "def L_pois(theta):\n",
    "    grad = sum(X_pois) / theta - 100\n",
    "    logp = sum(X_pois)*np.log(theta) - theta*100\n",
    "    return logp, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the slow algorithm on the poisson data is:\n",
      "CPU times: user 1.49 s, sys: 16.1 ms, total: 1.51 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run slow algorithm on poisson data\n",
    "samples_pois, burned_in_pois_slow = nuts6_dual_averaging_slow(np.array([2]), 5000, 2000, L_pois)\n",
    "print(\"The run time from the slow algorithm on the poisson data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the fast algorithm on the poisson data is:\n",
      "CPU times: user 1.04 s, sys: 12.8 ms, total: 1.05 s\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run fast on poisson data\n",
    "samples_pois, burned_in_pois = nuts6_dual_averaging(np.array([2]), 5000, 2000, L_pois)\n",
    "print(\"The run time from the fast algorithm on the poisson data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the data is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.9699999999999998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean of the data is:\")\n",
    "np.mean(X_pois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the samples from the algorithm is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.9855873668618411"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean of the samples from the algorithm is:\")\n",
    "np.mean(burned_in_pois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example we did was a multivariate normal.  We generated simulated data from a MVN distribution, and then fit a MVN model likelihood with an improper uniform prior. Once again, the mean of the sampled $\\theta's$ was very close to the mean of the data, so we considered this successful. We ran both the slow and the fast implementations of the code on a 5 dimensional MVN and a 10 dimensional MVN. The data was generated with means of 1 and a covariance matrix that is the identity matrix, so there is no correlation between the different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVN 5 dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate data from 5 dimensional MVN and create a function for the log likelihood and gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simulate multivariate normal data\n",
    "X_MVN5 = np.random.multivariate_normal(np.ones(5), np.identity(5), 100)\n",
    "\n",
    "def L_MVN5(theta):\n",
    "    cumsum = 0\n",
    "    grad = 0\n",
    "    for x in X_MVN5:\n",
    "        cumsum += ((x - theta).T @ np.linalg.inv(np.identity(5))) @ (x - theta)\n",
    "        grad += (x - theta).T @ np.linalg.inv(np.identity(5)) \n",
    "    logp = -0.5 * cumsum\n",
    "    return logp, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the 2 implementations of the algorithm below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the slow algorithm on the MVN d-5 data is:\n",
      "CPU times: user 2min 37s, sys: 953 ms, total: 2min 38s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run for normal\n",
    "samples, burned_in = nuts6_dual_averaging_slow(np.zeros(5), 5000, 1000, L_MVN5)\n",
    "print(\"The run time from the slow algorithm on the MVN d-5 data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the fast algorithm on the MVN d-5 data is:\n",
      "CPU times: user 33.8 s, sys: 246 ms, total: 34 s\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run for MVN\n",
    "samples_MVN5, burned_in_MVN5 = nuts6_dual_averaging(np.zeros(5), 5000, 1000, L_MVN5)\n",
    "print(\"The run time from the fast algorithm on the MVN d-5 data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean vector of the data is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.90158516,  0.97847045,  0.99262766,  1.01256208,  0.90676946])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean vector of the data is:\")\n",
    "np.mean(X_MVN5, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean vector of the samples from the algorithm is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.89403405,  0.98520147,  1.00344827,  1.00763917,  0.90151075])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean vector of the samples from the algorithm is:\")\n",
    "np.mean(burned_in_MVN5, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fast implementation of the algorithm was faster and that the algorithm outputed the means very accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVN 10 dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate data from 10 dimensional MVN and create a function for the log likelihood and gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test speed up ratio for 10-d\n",
    "# 10 dimensional multivariate normal data\n",
    "X_MVN10 = np.random.multivariate_normal(np.ones(10), np.identity(10), 100)\n",
    "\n",
    "def L_MVN10(theta):\n",
    "    cumsum = 0\n",
    "    grad = 0\n",
    "    for x in X_MVN10:\n",
    "        cumsum += ((x - theta).T @ np.linalg.inv(np.identity(10))) @ (x - theta)\n",
    "        grad += (x - theta).T @ np.linalg.inv(np.identity(10)) \n",
    "    logp = -0.5 * cumsum\n",
    "    return logp, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the 2 implementations of the algorithm below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the slow algorithm on the MVN d-10 data is:\n",
      "CPU times: user 2min 41s, sys: 1.22 s, total: 2min 43s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "samples, burned_in = nuts6_dual_averaging_slow(np.zeros(10), 5000, 1000, L_MVN10)\n",
    "print(\"The run time from the slow algorithm on the MVN d-10 data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run time from the fast algorithm on the MVN d-25 data is:\n",
      "CPU times: user 32.6 s, sys: 98.2 ms, total: 32.7 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "samples_MVN10, burned_in_MVN10 = nuts6_dual_averaging(np.zeros(10), 5000, 1000, L_MVN10)\n",
    "print(\"The run time from the fast algorithm on the MVN d-25 data is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean vector of the data is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.12751441,  0.96174488,  0.92994176,  0.97605577,  0.9607406 ,\n",
       "        1.06693436,  1.18451676,  0.92277773,  0.86999797,  1.13545702])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean vector of the data is:\")\n",
    "np.mean(X_MVN10, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean vector of the samples from the algorithm is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.12064651,  0.95581029,  0.93097548,  0.95586395,  0.97121095,\n",
       "        1.06685188,  1.1759705 ,  0.92149073,  0.87795877,  1.11895955])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The mean vector of the samples from the algorithm is:\")\n",
    "np.mean(burned_in_MVN10, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with other MCMC Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the implementation of NUTS with another MCMC algorithm, we test our algorithm against Metropolis by using pymc. We will use the 10 dimensional Multivariate Normal as our example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 5000 of 5000 complete in 0.7 sec"
     ]
    }
   ],
   "source": [
    "#pymc\n",
    "import pymc3 as pm\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "niter = 5000\n",
    "with pm.Model() as test_context:\n",
    "    mu = pm.MvNormal('mu', 0, tau=np.identity(10), shape = 10)\n",
    "    y = pm.MvNormal('y', mu=mu, tau= np.identity(10), observed = X_MVN10)\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(niter,step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is getting the unique values to plot the path of accepted $\\theta's$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metropolis_samples = trace['mu']\n",
    "# subset just accepted theta's\n",
    "unique_samples_met = np.array([metropolis_samples[0,:]])\n",
    "\n",
    "for i in range(1, niter):\n",
    "    if all(metropolis_samples[i-1,:] != metropolis_samples[i,:]):\n",
    "        unique_samples_met = np.vstack([unique_samples_met, metropolis_samples[i,:]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset just accepted theta's\n",
    "unique_samples_MVN = np.array(samples_MVN10[0,:])\n",
    "for i in range(1, 5000):\n",
    "    if all(samples_MVN10[i-1,:] != samples_MVN10[i,:]):\n",
    "        unique_samples_MVN = np.vstack([unique_samples_MVN, samples_MVN10[i,:]])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of samples from NUTS sampler with path of first 50 accepted $\\theta's$ plotted on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEJCAYAAAB2T0usAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee4aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(samples_MVN10[:,0], samples_MVN10[:,4], \".\")\n",
    "plt.plot(unique_samples_MVN[0:50,0], unique_samples_MVN[0:50,4], 'r')\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of samples from Metropolis with path of first 50 accepted $\\theta's$ plotted on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEJCAYAAAB2T0usAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnWd4VNXWgN+ZyUx6z6QRejmB0HsvASliR4pdUUFQLFdR\nP/V6rdgbWFAR8YIXEbAA0gkgiCEQEFLIBJJAAiQhhPQy9Xw/ZjLJpJEAIZT9Pg9P5pyzz551TsJe\ne6+19loKWZYRCAQCwfWNsrkFEAgEAkHzI5SBQCAQCIQyEAgEAoFQBgKBQCBAKAOBQCAQIJSBQCAQ\nCACnxt4gSdJCQKnT6WbU06YF8BkwFigDVgHP6nS68gsVVCAQCARNR6NWBpIkvQHUqQRsbTTAVsAH\nGARMAW4C3r9AGQUCgUDQxDRoZSBJUlvgOyACOHGe5vcAQcAAnU5XaLv/VWDWRcgpEAgEgiakoSuD\nwUA60A04fp62Y4EtFYoAQKfT/aDT6QZekIQCgUAgaHIUjU1HIUnSduBoXT4DSZJigW1AOXAvIAO/\nAK/odDr9xYkrEAgEgqag0Q7kBuAFPAKsB+4EWgBfAFrgwSb4PoFAIBBcJE0RWmoEcoH7dDrdAZ1O\ntxZ4BrhPkiTfJvg+gUAgEFwkTbEyOAWU6XS6qvanREABtAHy6rpRlmVZoVA0gUgCgUBwTXPRA2dT\nKINdwCOSJKl0Op3Zdq4bYOI8zmeFQkFOTlETiHR50Go9hfzNyNUs/9UsOwj5mxut1vOi+7hoZSBJ\nkhrwA87pdDojsBB4AvivbV9CS6x7DH7Q6XR1rgoEAoFA0HxciM+gevjRYOA01g1m6HS6M8BwrAoi\nFlgGrARmX7iYAoFAIGhKGr0y0Ol0kdWOdwKqaueSgAkXJ5pAIBAILhciUZ1AIBAIhDIQCAQCQdNE\nEwkEAsFVT9SBDJZtPgrAvWM7Etm7ZTNL1LSIlYFAIBDUws9/xPPQziW0z06xK4VrGaEMBAKBoBZu\nj/2NO2J/Y1zc5uYW5bIglIFAIBBUQ3nqJNNifwXATV/KvWM7NrNETY/wGQgEAkE13N98FSe9tTDj\nwFZuFF7j/gIQKwOBQCBwwCn6b1x+WYWxZy9khQJF0dWbpqIxCGUgEAgEFZjNeLz8PADF8z5A9vBE\nUVzczEJdHoQyEAgEAhsuy5ehjjtE+eRpmPr2R/bwQFlUeP4brwGEMhAIBAJAUZCP+7zXkd3cKfn3\n6wDInp4oioWZSCAQCK4b3D56H+XZs5Q88xyW4BDApgyEz0AgEAiuE5KScF20EHPrNpTNfNx+Wvbw\nRGEwgP7aL98ulIFAILi+kWV45hkUJhPFr88DF5fKSx7WojHXgxNZKAOBQHBdo9m6CTZuxDB8FIYJ\nEx2uyZ42ZXAdOJGFMhAIBNcvBgPu//4/UKkofutdqFaD3WJXBte+36DRykCSpIWSJH3TiPbrJEmK\nauz3CAQCQVPj+u1CnFJTYPZszOGda1yXPTwAUJYIM5EDtprGMxrRfiZwY2OFEggEgqZGkZ2N20fv\nYfHzg9dfr7WN7OFlbXsdmIkalJtIkqS2wHdABHCigfd0AN4G9lywdAKBQNBEuM97HWVxEUXvf4Kn\nry/k1DQFydeRmaihieoGA+nANGDF+RpLkqQEfgDeBSSg/YUKKBAIBBfL3sQsvl2bCMCjN3dhiP4U\nrsuXYerSlfL7HsSzjvvsyuA6iCZqkDLQ6XQ/Aj8CSJLUkFteAiw6ne5DSZK+vXDxBAKB4OL5fdl2\nPlrzHodbdudb+QHG/fkOAMVvvwcqVZ332UNLxcqg8UiS1Ad4Buh7qfsWCASCxqJKTOC95S/iV5JH\naH4mJ/3DUO+PQX/zbRiHDKv3XhFaeoFIkuQM/Bd4RafTpV3KvgUCgaCxOEX/jc+tE/ArySPdvyVu\nhjKe3Pw5sosLxa+9dd77K81EYmXQWAYA4cB7kiS9bzvnDCglSSoEuuh0upP1daDV1mW9uzoQ8jcv\nV7P8V7PscAXKv3YtTJkCJhMsXUqroCAYOxYAxfPP4987wqF5rfK3CgbAzaTH7Up7vkvMpVYGe4Hq\n9eHeAVoBdwOnz9dBTi0e/asFrdZTyN+MXM3yX82yw5Unv/NPP+L5zBOg0VD43+UYxoxDdTQZP9v1\nnIdmOUQP1SX/f76K4Utg964kFr66nk/m1G9Wai4uhSK+aGUgSZIa8APO6XQ6PZBa7XohUCbMRgKB\n4HLg+sV8PF5/BYuPDwU/rsTUbwAA7m9X7iVQZZ3G3L7uusaKokJUuiS6HvoTADdDKQUlxqYVvJm5\nEGUgVzseDEQBo4A/L1oigUAguBBkGfc3/4Pb559iDgmlYMWv9l3F6j934Lx+rb2pZssmytp3hOJi\nnI7q4PRx3GMO4KQ7gkqXhOqU1Zo929a+wNX7cj/NZUchy9XH9mZFvpKWmo3lSlsqNxYhf/NxNcsO\nV4D8JhMezz5p3TvQvgMFP/+GpWUr+zW/Qb1RnThO+R134vLLKgDMrVqjSq+5h9YcHIJZCscU3oUf\nT6nQebTgWFB7PLxcr2QzkeL8rernkoeWCgQCweUiv1jP6o3x3P7Nq2j/2YWpcwTFb76DOiYa1dIl\nOCUdwXnjH/b2FYoAQJlzBsOwEZikcNz69SYvtC1mSUL28bW3ue2yPk3zIpSBQCC4alm9IZ6b5j2O\ndMq6u9jpSAI+d95Sa9vyW27HOHgozmt/Q/PXLgo//wbDzbcC4Kb1xHQVr8wuBUIZCASCq5aQdB1d\nbYqgzM0Tp25dMUmdMYWHY5Y64/r9IpzX/U7xG/Moe+wJAEy9eqMZNwrN1k12ZSAQykAgEFzFDJ5x\nB1+7ulPm4c3E2wfi41lZpUyVEI9m/VpMHTtR9vBM+3lTj15YtIE4b9lEscUCSlHWBYQyEAgEVzE+\nni7cMbuW2b0s4/HKCygsForffAfU6sprSiX6MWNxXb4Mp0MHMfXqc/kEvoIRKlEgEFxzaNatQfPX\nLvRjx2OMvKHGdcOYcdZ2mzdebtGuWIQyEAgE1xZlZXi89jKyWk3JG/NqbWIcOQpZrUazdfNlFu7K\nRSgDgUBwTeH25XxUGemUzZiNuV2HWtvInl4YBw5BfeggyuysyyzhlYlQBgKB4JpBeeokbvM/xqIN\npPRfc+ttaxhrMxWJ1QEglIFAILiGcH/zVRRlZRT/+3VkT6962xpusCmDLZsuh2hXPEIZCASCawKn\n6L9x+WUVxl690U+567ztze06YGrfAc2OKNDrm17AKxyhDAQCwdWP2YzHy88DUPzWew3eO2AYMw5F\naQn8KXJsCmUgEAiuelyWL0Mdd4jyO6faU1Y3BMPY8dYP69Y1kWRXD0IZCASCqxpFQT7u815HdnOn\n5NU3GnWvccAgLB6eVmVwZWVwvuwIZSAQCJqM9Owi5n65h7lf7iE9u2kSwbl99D7Ks2cpeeY5LMEh\njbtZo8E4ajSkpqI6drRJ5LtaEMpAIBA0GQtWx6FN+ofef61l7fzVUF5+SftXHU3GddFCzK3bUDbz\n8QvqQy+iioALyE0kSdJCQKnT6WbU02Yq8CLWesinge+AD3Q6neVCBRUIBFcfI/av54F1C1DaCiTK\n3/8Lc6dwTN26Y+reA1O3Hpi6dkP2uIAavrKMx79fRGEy8W6PaUR/usd+afGLkQ3uxmBLV6HZspGy\n2XMaL8c1QqOUgSRJbwAzgEX1tJkALAOeBDYCvWztnYC3L1hSgUBwVeH6+Wc8tG4+hW7erBl+F2N9\nyvE9lohTYjxOifGw4n/2tqZ27a2KoVsPq6Lo1gM5IKDe/jVbN6GJ2oqpXXvy3XwYnLyHk35hpAe0\napSccmAg9O+Peu/fKAoLkL2u/RKXtdEgZSBJUluss/sIoGadOEdmAit1Ot1XtuM0SZK6AA8hlIFA\ncO0jy7i98ybun36IObQFppW/M65jJwDyAcxmVCnHcIo7hFPcYdvPQ7j8/gv8/ktlNy4uWLSBWAIC\nrD+1gaBUosw8jSojHSddEgBOqSl8kPoiACmB7Xj63o8bL/PEiShiYlDviMJwy+0X+wauShq6MhgM\npAPTgBXnafsmUFLtnAz41tJWIBBcS1gsePzfc7h+vwhT23YUrFpTWYu4AoMB2ckJS0goJoUCizYQ\nU5cI1Pv2oj4Qa2+mKC9HlZGOKiO9QV990rcFX0XWab2un5tugv/8B+fNG4UyqA+dTvcj8COAJEnn\naxtb9ViSJC/gMWDDhYkoEAiuCvR6vO+ejGbXDuvhpCm4LFuCKjMTZZbtX2YmyoL8OruQVSosgUFY\nQkKwBIciOzmhPJuDMueM9V9+3ff633QDr/b1wtQ1ALPBABpNre3Ss4tYsDoOgDmTutEqyBN69cIc\nFIwmaguYzaBSXfh7uEpp0uI2kiS5Ar8BLsD/NeV3CQSCpkNRVGgdyLMyUWaeRpmViSrzNMqsLJRZ\npzGmpqLNz3O4x/3Ddx2OLd4+WEJCMPXqjSU4hDxPfzaftpDn6c+42wYQGNEBS4DWPhDXOmibzWhD\nHI0MsloNFguuS5fgunQJAEaVEyf8W6Hp2xvf4YMwdetOnHson61PwWCqjGNZsDqOD2YPBoUCww3j\ncF32A04HYzH17X+pX+EVT5MpA0mS/IG1QDgwRqfTZTTkPq32AqIKriCE/M3L1Sx/s8huNEJmJpw+\nDadOVf6renz6NBQXN7jLjd1u4LRvC7RdO3DzlKHQogWEhqJ0c3OIZX/5rc3keJYBcPioisVTO9qv\nnSss551lB9AbzQB88Vs8i28MgD5VqpItWgTTp5NXpOeHVfsJyEihcPde7oxZBSholZuBZn0qrF8F\nwBCFkja+oUR3GMjSofcCoFQp7O/dddJtsOwHfPfsgAmjG/0qr3aaRBlIktQG2Ay4A8N0Ol1CQ+/N\nyWmajSmXA63WU8jfjFzN8l9y2WUZRd45+8xdlVkxo7ceKzMzUWVlojibg6KenbeWgADMbdphCQ7G\nEhKKJTjE9jMYc3AouLrg+fgM1Adi+bv9AD6Y+CxGJ6t5ZkCXIAZ2ibB2VGKGEsfns5hlh89Vn//r\nNQl2ReBiKGPKHz8gv/obCtv1gkU/8Iu2J788t6ZKjz64hA/n8a3W2JXbnlpFq7yTvNXPBaf4w6T9\nsZMOmUeZErOKnwZOwcvPi8dv60pOTpH1/fcYQIBGw9llK5le2heACQNbMnlkR6qTX6xnRdQxAKZG\ndsDHw/n8v5Mm5FJMJC65MpAkSQtsBwzAIJ1O1zDvj0AgaBjl5VYzTYXJxm6TP11pn8/OQlHPBi/Z\nzQ1zcAiWjp0qB/iQEOu5YOtnS2AQONc+yOUX6/njl2ju/uQp1KdSyb7pTt7veBdmhYoOLbzw93Zl\namTthWUqmDOpm4MZqDb6pcQwK+obtEVn7eeiw4fwdqI3kFbzBnd3+8eA4lwyQ9sz3zWAqS9MImd6\nMWV3TaZv6n6eub0L4RHVHNseHqR06EmHxBj8inI55+nPhuiMWpXBiqhj7E3Mth/PvCWi3me9Grho\nZSBJkhrwA87pdDoj8KXtOBLQS5IUZGsq63S6Mxf7fQLBNYvFguLsWVS2mXuFbb66fV6Zl1dnF7JS\niSUwCFN458pBPTgEc7VZvezlDQpFnf2cj40/7+LpF+8AICZyMur356Ncsg8lcOvQtkS09a/1vuoz\n6g9mD6613d0RHkz66iU6x25HdnKidMYsnFcsx1RSyrdD7q/1ngFdgpg6sh28Zz2eEv0zC8bNsQ/a\nM2+JwKtTIKRC55be1LYeigrsTofEGPqmxbK5+9iGv5BrgAtRBtXf4WAgChglSVIMcDugAGKqtFEA\nJqB2975AcK1TXFxlJn/a0XyTlQlnsgjIzERhMtXZhcXLG0twMKZuPSsH+SqzektwiDUW36lJ40JQ\n6ZKYY1MEAJvveoZ/fthvd8wuWB3HwudGkpCWy4LVccgyuGhUaNQqQgLciE89Z7+3xozabMblh8X4\nvvkaTiVFJLUIp/DDz4jYuwVVQT4rB0zhjHcQ1bmviwsTdWtweW2Z/VzH7GO1CG99Nz+sS6DMw7uG\niWd/2z7M2LGIOVu/5Ix3IP+07lnrO6i66qltBVTx7GBd9dSlHK8kGv1Xo9PpIqsd7wSqxmE17V+i\nQHAlYTKhPJNdaa7JzqzVPq8srtsfIKvVEBKCqWdvLCGhmIODK2f1Ffb5oBDw8LiMD1Y7TrH78K3i\nXN1104NMHd2Rf47l2s8ZTBZW7jjKtv2n7ArCWGaBMiNFpYY6+1YlxOP53JOoY/dT4uzOwjGz2Nzt\nBsK2n+CLHxZQ6BvIqv6VSsjJZGRAagxj47fS65N/UMgyFncPTnsHE1qQRWyb3va2PTtYB2PZpij/\nOZJFvrvVjDbz5i4cXLqW9M8W8U7Sbvs94+I216kMfDyc6zUNLVgdV0M5XumIgVsgqA1ZRlGQX2mu\nyc6ymmpsA779fM6Z+h2wfn5YWrXGWDFzrzaTNweHIvv7ow3yJv8Kd36ro7bgM22S/Vh2daXz+68i\nezjz8kP9efWbv+3XNkRnoHGqmQfT3UVNOz9XkjPy2ZeYTazuDC4mA+/lbqPl/xahMJkov30ST4bc\nwhkXa1qI+7cvRqHXU/L+W/R0b03AqVRG/bOJ4I2/4V1WCEBZn35s7TaWxL6RxB7J5qcv7kHKSrZ/\n7/frkxjQJdgetqq0mGl19gQjf/0Nv7d2oj2eRi+g2LnS5+CmL72k7+9KRygDwfWHXm/fAKXKznSY\n1Sszbfb57CwUZWV1diG7uGAJDsE4cLA10ibYcYC3hIRgCQoGF5fL+GCOLN2cxPYDpwEY1TuU+8aG\nX3Bfzj8vx+uJmQ7nyu65354/qJcUWOOeCgdxVTPRnEndmLc0FotNf/ZIOcDsbQsJLsjG3Ko1xe99\nhGH0WO5Ly+WzlYfocvwQA49GUxoegYdZzwtfPYk6dh9gjXQqfXAO5ffcz5dJJqtvILUIbZAfGf4t\n6ZCdgtJixqKsNFyoTloj3L9e9iQuZdZECbKbGzvDh7NTGsrB1r34ePlc2uYcp5uPolEJ72p79orP\nVwNCGQiuHSwWFDk5VgdsVqbDrF6ZabPPZ2eizM2tswu5Ij1Cp/BaBvnKWb3s7XNRDtjLQYUiqPh8\nocrAdcGneLz5KgClHt44BfijPplO2SzHDJ+DuwWxJ87qrO0bHsC+vccI8XfjRHYxxjIL9w5rY904\nBviU5PPIju8YoduFWaHk1/53MHTFF/ZooIi2/vi5OvH2qv8A4JaUAP+ag6xQsL9Nb7Z0HUNcxEA0\nrm7M8Q7Fmi3HSn6RHl1IJ1rnptMqN50iN29ecU3FZ/zr9nQXLmUlRLfvz87w4YTeP4nWHcOI/dF6\nzc/bFXKw+nJkGRSKRvsAItr6XxWmoaoIZSC4OigpsQ3yWbXb57OzICuTAKOxzi4sHp7WHbBdutrN\nNTXs89pAUKsv44NdWexNzOLbtYkAPHpzF0b/9wNcF38LwJEQiV/63c7La96lfMpd9pxD+cV6lmzS\nsf9IDmDdFzBs/n+IjN/G+zc+y4nwYQAs23yUyJ4tmKeKp8WSt/HQl6AL7sjnN8xm0hO3gbs7UQcy\n+HXNQcYkRPHdzsV2ubK9tGzpOoZtEZGc9dRaTxoBYzkLVsfx4ASJg8nW7zeYLGT6WIvcLFj6DBYU\nKJGRq9RF/tdd73M0pJPtobPRJhfy6oP9aOXlhNdn1iI3quwsVMk6zFK4gw/goxWH8PdyqdwVfY0g\nlIGgeTGbrQ7YajP5Gvb5woI6u5CdnKwmmd690QcEYa5hn7eFU15IzvyrmFG9Qx3MRPWRX6xn6SYd\nB49WxvMHP3QXrmnW2fJf3Ufz7ugn+GD5CwCUPvG0vV3VmPu2Z1J5/o+PCMs7BcCkfb+wSxoKCgUt\nczM402cQEaeOUKpx5avIGWzsPg5fH3ci2vqjzDyN5uWXWHx4E27Gyj0SH054hj/DhyEraq/FZbHI\nLNmgQ1VawsBj0QxP2kXf4wfs15NCw9kZPgyXuyYz7a+fcF3yHeUaV4c+cvLKeGfZAdodj+d9k4ky\nD29ciwvQbN5ImVRzRZVbWF6ZyuIaQSgDQdMgyygKCypn8tU3SVUM8meyUVjqrnlk8fXF0qIFpj59\nbbHyFTP5yl2wslYLSiVarSeFV7gT9nJy39jwBpuGVkQdq1QEssxXSx4nLM+qSP68+WE+6HgzXTPi\nCM9KpviGCZjDOzt2IMvceGgDD+/8Ho3ZyM6RU3DOOM7AlBi6Z8TRLSOOSft+RW0x8VfHQXw76hHy\nPK2mlgfagcczT+Dy83JuNxrJdfezK4NFI6azs/OIOuXWGPWMPXUYac9muifH4Gy2RiulatvQLuc4\npWoXXpj2jrXxkRKm2qKJxvYO4btqyfj1RjPhmToAlvSfymPbv0WzdRNlc562+wCq5jW61hDKQNB4\nDAabHd6267U2+3xWJorSuqMxZGdnLEEhmPoNsM7kg6pH2Vh/4upaZx/XGpcjxcH5bN8K2cKaTyrD\nN4s+/YI/PXpDYjZ3xlhrDbweMJIRiVkM6BLMl78dJulgGi9t/pxBKXspdPHkg9teRJ5wI6UbtzIw\nJYa3V1n9DTmeAXwVOYN97a1J4DpmHmXSvl8Y9FE0SmRM7Tuwotet7NeE8OHy58n0CeGPnhNqPIPK\nbKJn+iGGJ+1iYMpe3AxWR/9JvzB2SsPYJQ3llF8L3lr5b3pkxOFeXkyJiy0s1xZNNKJrEEPu6mFP\nhqdUKcjJK0OyKYN97foxJnkX7fdG8+xb6yhx8WDOpG54umkuiWP4StyHIJSBwIHp70bhXl5M+Okk\n/EvymN7TyyH9sCrrNMqzZ+vtwxKgxdS+oy2iJsQxXt5mn5d9/a54B+zl5nKkOKgr/n1qZAdM5Xpe\ne2KkvW3+T6sxRt7A1GI9wSd09DlxkMNhXUkKkUhem8iALsGUbNvF/PUfoS06y+Gwrnw04RnOefrz\nYpgafcI2e187pWF8fsNsytUu9DzxD1P3/0LXE4cBOBrUgVX97iC6wwB8vF159rvnUckWvh35MCaV\n1X+jkC1EnExkuG4XQ5L34FVuXQFme2lZ32MCf0pDSdO2dfib0oV0okdGHB2zj/FP6564aFRQbhvy\nbJv7WgV50iLQjcPHzoEsE35aR667HzmeAUS37k3Hk0foeiyWXeHD7O/rUpiGrsR9CEIZCGrwyu/v\n0PWULbfglsrzsps75pAQTOFdsAQFV8tnYxvwA4PqzCMvuHLxweigCObe/zGpB9TMaZtLRFt/7j1s\nTQi3qr9tn4HZzMbbZ/HOnuUALBt8Fyv734lFoWR0wja6fbnEPmADnPINpc/xA0ze9wvts1MAONi6\nB6v63cHhlt3tg3h47A66n4wnufsQDrbtRcesowxP2sUw3W78S6w7l/PcfFjbcyJ/hg8jKUSqc1KR\nHGx1EHfKTOaf1j0pN5j5Y98pJgNvf7+X0Bx37hsbblUEgLboLP4l59jfeQgoFOxr24f7/vqRfmn7\n2WVzgjeWK3EFUBdCGQhqkOvhB8CKAZMZ99y9lQ5YTy8xm29Czpfi4FJQPf49v1jPul9jeObZifY2\ns2Z8w0mPQDBZ+OTnQ4z1MzB77W8UdOzCoTY98S85x9MbPqVn+mHOevjz4Y3/IiEsghbnTjF761d0\nPxlPmdqFb0dOZ0f4CH5c+AB3R1sLJFpQsLvTYFb1u4OUIMdn1Bj1TP/zewCyy2Hhd7MILcgCrJvB\nNncdw87wYcSHdbXvHfBwdaK4rPYUHjpbtFB4ZuXmM7MtokhlMdcIt60wER3WdsTVWUVh+86UBgTR\n5/gBXJQyj1+AWaiuFcCVuA9BKANBDaI7DGCEbhcmpQrjiFHNLc51w/lSHDSW9OwiPlt5mJJyI51a\n+TD9xs414t+/XpPAC8/faj8+m5TGmcVxYBvALDK0/+lbFBYLiuef5wdPC15PPI/y7Fn2tuvHZ+Pm\nUKZ2ZdrfK5gSsxK12cTedv34OnIGOV5a7thXWdO40MWTuXe9y2nfFjVkDcrP4osfnrQ7gIcl/0W5\nkzM7pWHsDB/Gwda9MDnVDPnVG+p26Oa7+5LtpaVTVrJ9v4DZpkSUssX+/AqF9XKF81gX0okyvZkv\nnhmBIvVGvJZ+zzcjPVh+4hwfrTgE1Ext3Vh/T237EJp7FVF7rJbgumXxi5E8PP9pZLWaKcVHmlsc\nwUWwYHUcecV6DCYL8ann7INVdZYNvpu0gDa8++V2ZD9/5kzqhsZJiVIBfsXnGJ24nXz/YJxi9+Mz\nbRKKwkK+G/0Ib936Eq1yM5i/7Bnu+Xs5hS6evHPT87x160vkeFn3AlTN7eNVXkSWXwv6hlt3LfsV\nn+PW2DV8+L+5LFr8mF0RHGrZjfcmPse9s37gw4nPsq99/1oVAYDRXH90T3xYV7zLCuljC5G1KCpW\nBtb79iZmU5FNRMpMxqRUcSywvf1+w9jxAJT/9jsboivrc1X9DJX+nr2J2Q7vueJdapyU510BfLTi\nEAaTBYPJYlc6lxOxMhDUQPb0wjhsBJqorSgz0msWNBdcVhqSVqIxs8qqs9gJA1qxgVl8b7QWkl+w\n2urU7dVJy4QBrTA8Nxe12YT3uWwUCz+npEVrXh75JNlegczZ8gVj47diQcG6HjeydOg9lDq74+/l\nQm6hNTQ0NbAdz971Ph8tfx6AWVu+xHKmJ4/H7cTvnxiUyJir7B/4duR01vS+5WJfmZ1f+9zK6MTt\nTIv+mdi2fezmJZXF7NDOyWSk/ZkU0rRtMaiduXesddZvGDoc2dkZ49o/4J4xNd7jhawArlSEMhDU\nin78RDRRW3He+Adlj85qbnGuaxqSVqI22/ScSd0czEQ9O/jz2Ic7MJkt9txAYI1a+npNgkMkE8Dp\noyf5ZMdvAChkmfI7p/JwyG30PRbDa7+8gU9ZAWkBbfhizCx0oZL9vuIyx8ykGf4tyfEMQFt0lvGH\nN8PhzQCktu3GpraDyHP35aW175Ec1JG1vW66uJdVjRPaNuzpMJDBx6LpdeIfu5lIJTsqg/ZnUlGb\nTehCJbp38GPZ5qMs22zdifx19/602rcLbWGOfcUD1tVAhVnvcvh7mhqhDAS1Yhh/Izz/DJqN64Uy\nuEppFeRtJqpIAAAgAElEQVTJR08MsR8/9uGORm2a6rl3I876UvRqF74aPYMj3p15fs179D7xD3on\nDd8Pu581vW/BpHIcRvRGCxqjnr5psQzX7aJvaqzdBFTBj4Om8dOgaSgtZj7+31wAvhn1cJ27jC+G\nnwZOYfCxaKZFr2BPR2tYqLLKRkdfD2e7vyApuBO5MXGMPxmPq74Ud0MpQfv/AmDxokc51LIb8WER\n/DRomsN3XAp/zx0j2vLLzjT758uNUAaCWrEEh2Ds0xf1nt0o8s5Z9wUIgCuv/i00PjpFqYB+nYOY\nGtmBhLRcDiTnoFRA2xAvUk5b00KnBrZjV6ch/DRwCv1T9zF/yVM4mw3EtunNV6Nnkl2tyEzFZrD7\n/vqR9mdS7ecz/ML4UxpGcEEWoxO3A3DP3z+R5+6LrFDS/kwq2zuPQBd64VlVa3u2ipVOWmA7otv3\nZ2BKDAWu1rTYVc1EnVr52COJkkLD+deGT+hyOqnWvntkxKGQZX4aBMnp+aRnF503P1FD/l7Ss4vY\neTCzWXMeNVoZSJK0EFDqdLoZ9bTpC3wK9AJOAm/pdLqlFyyloFnQj5+IOnY/mi2b0E+5q7nFuWK4\nEuvfNsQ2XV1hRLT1J79Yzyc/H7KbjTLOFNvbJ4RFYFKqmLv+I9qcTSfPzYfPRs6x5xqqitpk4Ksl\nTxBUWFnZdlW/Oxw2g3mX5jNctwu12USBqxdP2ArXl6ldWDK0spSlWqU8r2O4PmQZjp0sQKVUYLY9\n2IoBUxiYEsOglL1AZTQRQEm5kfBMHXlu3mR7BbKq/yRe/t2awmL+2CdI07Zh/rJ/AXDaJ4T3brKu\nZPKK9Q3KT9SQv5cFq+PsfpbmynnUqDWZJElvAHUqAVubAGAjsB+rMlgAfCdJ0pj67hNceRgmWO23\nzhvXN7Mk1zcVzszqnxtLRFt/3n1sEL06adkdl2WfsVb1H1SYkdzLi5m1dSEfrPg/2pxNZ2P3scx6\n8HPr5qta9po4WcwOiqBM7cLygVNJC2xnbx8xQGJ3J6vZ6qeBU+xtSzWunPOsdHhfjCIAa13e3MJy\n645jG8eCO7CvbR/7cdWVQU5CCtqis+hsG9j2tevHN6MeQSVbmLJ3FZP2/Wpvu2LAZNTBNctuXgs0\naGUgSVJb4DsgAjhxnuaPAvk6na4irWGyJEm9geeArRcqqODyY+7YCVP7DmiitkJZ2XWVJ6g+Lrez\nMLJ3SyJ7t7zg+6uaKQxGs0Nm0hrIMkOT/+LRHd/hV5JHWftOfDx8Bkdbd+X2gS3tTtXqlFXJAmpQ\nqXE1ltP7xEGiOwy0n98Tl01ujxsZdWQnEw5ttJ/3L8nj1tg1/N7n0kURARhNFp6d2sPuXF8xYAr9\nbCGmPqX5uBrKKFO70D7DGkKtC6l0gq/veSOBhWeYtP83WuRXOvCHlqczvJEmuYb8vVwJm9AaaiYa\njLV6xDRgxXnaDgX+rHZuB/BFoyQTND8KBYbxE3H74jM0u3ZgGFszadj1yKXeHNbUVDdTVKXq4JT2\n92Eei/qGfmmxGFRqlg65h1/63mbND1RYXqciAKuZqIKFkTN4cssXDNX95aAMwLqh62hQezraUlIs\nGXo/Nx9cxyM7F6OULfza97aLeVQHKlYYC58byfR3oxwinh7Z+T2P7Pwes0KJymYyemD3Uvql7qdU\n40qpsxvDdZX1kONbRNDm7HF6p8RyLtCjUWachvy9tArytPeZX6zn6zXWdDCX0yfVIGWg0+l+BH4E\nkCTpPK0JAw5UO3cacJMkyU+n051rrJCC5kM/4SarMti4XiiDawilonKgmXmjhOvXX6JZ+hYaQzn/\ntOrOl6Nnkekb0qC+wgLd6Lf+N/vx8YDWZHoH0T91HxqjHoO6ymCmUHDGK9CuDFb3u509HQfx9spX\nmP7nElQWc2X+o4tElqmxeeuPHhOYeGgDAPva9sXNUELEKevKwKh0QsrU2ZVDVbqeSiC+RRe6nkyk\nNPYf3Pr2AmDljqM1NqDdO7bjRa3kmssn1RTRRG5AebVzetvP5isIK7ggTH36YtEG4rxxPcUffGpP\nAXy9cyVGFNVFxex/35Fsu3/ASaXEx8MZp4OxeDz7FOr4w1j8/Sl8/TN+0Hcg81TD60IUnchiWsxK\n+3HLcyfZ3Wkok/etZmjyX8S17Ia7vhiP8mJ8SgsYcvRve9sntnyJs0mPXm0dGh7YvZQHdi/lnLsv\nhS6eFLp6kRAWwbYuo8j2Cb7od3HWM8D++aeBU0gNbMuKz+8mw78lT9/7MQrZwtMb5xN5ZAdGlRPz\nbn4Rs1JFmcaVoIJsup5KJGXRT3SzKYPqigCsFd3+t8W6inr05i4ADtXjBnS5+OdoCppCGZQB1f9n\nVByXnO9mrfbqrkZ1Tcp/6y2waBHalAQYMqTm9SuIy/X+l2zS2Wdvzs5OzL23LwCppwp463trxMor\nDw2gXQvvBvfZVLJrtZ688nAAB3VnePUb60CsKikmeuxd3HRoPQpZJuPGO3mh9S0UpXoBlYpAIVtw\n05fioS/Bo7zYNqiX4KEvwb28GA99MVNiVjt83zOb5tf6uTbGxW+p9bxfSR4ak4E2uel0PxnPXdEr\niG8RwdaukfzVcXCNSmUNpX9KjP3ztOifWT5oKhqzkaQQCYVsYda2r4k8soNUbRteufMNily97O1P\n+YZiVijpFP/3eX9XFUr327VHahzfNKL+IIDZk3vivCYegIdv6Yqf1+WZQzeFMsgAqq8vQ4FinU5X\nd+1CGzlXcaUqrdbzmpRfM2os3osWUbp8JSWdujeDZA3jcr5/vd7k8Lnie99YFG0PEXxjUXSDbcsV\nslcUWwEuebx5mJ91AB14LJqZUd8SUJzLSd9QPBZ/y+xdjrWjn13/MX3SDuBmKK3VbHI+MvzCaHnu\nJAB7Ogwk18MPtdnE+Djr7uOfBkxh2t6fKXdyZtZDn1Ps7EG52gVtUQ5vr/w3IQXZrOt5I6v6T2Lw\n0b8ZnRBFj4w4up5KYGbUt+zpOIitEZEkhEU0aqNaZ9t+AoB+afspdLW+X11IJ2Zt+5oJhzdh7Nqd\no29/S9HOTId7i1y9SAkLp+Oxw5zVHUf282fCwJa1rg4qkWscN+Rv9MFxVnO8WW8kJ6fuut4VXIqJ\nRFMog93Ag9XORQJ/NcF3CS4DhmEjkd3c0axfS8mrb4g01jRdRFFTxpsrT53k5d/nMTAlBqPKif8N\nnMrK/nfyzZBhsCvKoa2LsRxPfeWegwOte5LhF0apqwdDhnfBJyyIT7ec4MnNX+BTVsDCUY+ypesY\nlnzzMAVuXsx+8HPu272MKTGr2Bk+nD2dBvPkpgUAfDl6Jht6TKBdTir9U/fjXVpgL3Kf4xXIS1Pe\n4u2VrzJt70pUFgv/HXov27uMIrAgm8jEHYxOjGJ04nZGJ24n2yuQbV1GERURWWMTXHX8iyqjqGJb\n96LPiYP2TXAVyiY1sC1eq36nt58/r7ULY8HqOIpKDfaQ279b96FTRiKmDZtQ3XM3k0d2dMheCrA3\nMcvBLATUOL4SuWhlIEmSGvADzul0OiPWENS5kiR9BXwG3IA1CmncxX6XoJlwccEQOQbndb+jStZh\nrqVA+PVGXREizRUiGHUgwx7tU+HAXPRHAnvislFazDxxZjejf/+WgSXFxLeI4IsbZnHSL8y+b+He\nsR3t9/cND+Bt+f8YdCya+3cvJSzvNL1ykpEmjaXs8SfZdrSAZZuPEuF8Fp+yAhJDO/NHzxtBoSDD\nPwwpMxkns5Fd0hCmxKxiWPJucry03JCwjbSA1mzqNhaAP3rcSP/U/Uz8ZwPzx82xP8tZTy3/N/kt\n3l71bybvW43KYub74Q9wxjuInwZNZcXAyXQ5dYTRCVEMTf6Lu6NXcHf0Cg6HdWVbRCR7Og6q1Yw0\n6Fi0/XN8ywhcjWX2ncZWRdCO6I++Z/Vi6+/PRaOisNRxVr6vbR8e2L2UrGWr+LSgDVBzBTegS7Dd\nL1DhW6rY7V3Vt3Sl+Z0uJBFI9XXPYKzRQoMAdDrdGWA81g1nB4DZwH06nW7nRcgpaGb0E6zFTzQb\n/2hmSa5sKkIEP5g9+IJMPHMmdcPfy8WelqChVA37rPi8Jy6bdmdS+XD5C9zwv09A7UTRp18QFPsX\nEx8ci1IB/9tylL2JWUT2bsniFyN57aF+pJ0uxt/blXHznsQ5Po7D/3qDPIUz7h+/j2ef7uS/9wlO\nJiPTdy4BYOmY6aBQoFLCKf8wVLKF0LxMjge04aRvKP1S9jNnszWy/JtRj9gzhx5s05PTPiEMT9qF\nZ1mhw/Oc8/TnpclvkeEXxh2xv/HIzsVU5JqWFUoSwiKYP24O98/8nk/GPcnhsK50PxnPM5vm89+v\nH+KpTfPpmhGPooqJa3AVx7XSYrErJYCUwHZ47YpidVyBPY10dUUAcCKgNWc8tbRPiCYvv4TcwnLm\nLY0lv1hPQlouj324g8c+3EFCWi5Qd2rr811rDhq9MtDpdJHVjncCqmrnYgDHAGPBVY3hhnHIKhXO\nG9ZR9tSzzS3ONUvVePPzse7vNHtisxqUlDB95/fccmAtKtnC9vARdFu9GFlrNcd8sybRPqv7Zk0i\nUitfVkQd42ByjkP20w9mD+bfiu6op3/FrbFrmLT/V2ZuX8TM7YsA2NVpCPEB1tWFAkj3DQMg7NxJ\n0gNasbvTEKbtXUnbs8fZ3XEw8S0rFZysULK+xwQe2bmYMfHb+LXf7Q6PkOfhx0u2FcKtB9aitJj5\nZtSjDmbKco0rURGRREVEElSQzajE7YxJiLL/y/QOIqpLJEeDO9DlVGV9DpXFTKmzm/34lUmvMzO/\n+jy3ksHdgtgTlw0KBYmdBzAyZh3hp5NIDIvAYLLwyqJoSssrdzVfKXWNG4MobiNoELKPL8bBQ1Ef\niEWZlXn+G64TapsNno+KTUVfr0kgv1h//hvqoC5FMF2RCj26c3vs72R7BfLvSa8R/cK7dkUAjst7\nGXjhq7/Zm5jtkNW0qNRgl0+vduHngVN4dPpCNlaZUfdIP0yPE9ZYfpPF6jgGaHnO6lTdXyUFxPfD\nH6gha1SXkehVGm48vBFltRoDAPnuPrw0+U3SAlpz8z/rmRX1tcNsvyrZ3kH8NGgajz68kP+b/Cbb\nuozCtySfe/5ezmu/volKtnDa22q+UckWJFs5zJfvfINiV08WrI6zF6OpilIBg6qEg+4MtRbsqdjN\nDDgogqpMjezAgC5BDOgSZPctpWcXMffLPSSn59O1nZ/DteZEZC0VNBj9hIlodu1Es3E95Q8+3Nzi\nXBHUVeO2PppqU9HoUBUPR32L5/o1mJQqfu5/J78Omcr8/6vpruvQwotjp6ymGZVSUWs+IIPJwtJN\nOh69uQtfr7E6QAvdvMnyrhwYvcqLeGv1f9CrNKwcMMke2XPvnuWozSam7q3cfzD3j49QyjJuhlJc\nDaW4GcpwNVqd5cEF2XTITiHZVre4KgVuPrwy+U3eXPUqNx6yKo0vx8yqM4pIViiJb9mN+Jbd+Dpy\nBsu+uh+N2WryqaiprDKbCM9MxqxQcjS40gEc0dZa6e2TFYewYF2EtA3x4pOfKzevHW7ZDb2Thn6p\n+/lh2P3URqsgD3vxm+q/36pBAkqlolmS0tWGWBkIGoxhvNVv4LxhXTNLcm1T18qh+iqkIue9QrYw\n4dAGHn1pKp7r15DUIpyn7v2YpUPvpVihrnXVMvv2bvYZq8Vmi3c2ltvt8hWkZxczoEuw3TLjZDIy\nOWZVDZmdzQbu3bOc+/760X6uqiIACM9KpvXZE7jri9GrXTjlG8rhsK5Et+/P2p43ctLPWhtZU8sU\ntdDVi5fvfJNjge0YH7eFOZu/qHUlUZ3exw+gMRtJC2jD4/fPZ/nAqSSFSBxu1Z2O2UdJD2hlz6vk\nXJTPjLc28ZFNEYD1daScLnRM5qd25nDL7rTOTUdbJTlfVY6dKrwi/ACNQSHLddvJmgH5WozTv1po\niPw+o4fhlJRIblIasqdXvW0vN83x/i+kiHltUSRVZa9adWxAlyD7zLJqcRqNk5KFz41ElZhA6fRH\nCUuNp9jZjT+nPIHb7Jks+DXBweRT0b42pr8bxR37fuGhXf/FpFRR5OJJkasnhS5e6L28UWi15Dq5\nk2HWUKxxt6eNyPIOIts7iGK/IMJzUgg7nlhr/zvChzMy6U/2te3LG7e/ct7307mND0eO59d6zb28\nmDdXv0bH7GNEdRnFZ2OfsDukqxNUkM1nS59BZTHzzL0fcdIvDCeTkdD8TIbpdjHNpqyOhEi0yDuN\nV3kRScGdmHv3++eVcfzhTTy+9SsWD3uAX/vdTusgD/y8XGokAezewY+n7+zpcK4p9pJotZ4XHe8t\nzESCRmGYMBF13CE027agv+3S5JC5mrmQGreXItGdxqjH/a3XcP1yPn4mEwl9R7Nl2tNMvGMgPh7O\nhGrdOJ5ZXOO+upRXcnAnSjWuuBnK8C3Nx9lYTljuSZSnZDhSoxsA2p49QduzJzCnKil09XTYaFaV\npBCJkUl/0i9tP+2zU8j2CqTExb1OM09digCgxMWDf096jdd/fYPIxO0oLWY+Gf8UTho1FlnGZJZB\nlgksPMOi72YCUOzszsM7FtMi7xSBhTk1NtF1zD5GlncwiS0621Nsn49dnYbwyPbvmJi4lZtXzQeF\ngvxiPUazhfjUyvRrh4/VTMXWmCCBy4lYGVxCroeVgSo+Dr/IIZTfPomir7+/TJI1jKv5/VeVva74\n84qBvEfaQebu+Q7XU+mYW7ai+L2PMIxx9AtMf9dxE9mzU3sQ0dafmR9sx2i2/p9XqxR8PXcUn676\nh8PHzhGSd5oX1n1A+5w00gLa8MHEZylw88azrAivskI8y60/vcoK7ee8yovwqnLds6wIZY3o85qY\nFcrKFYhtFWL97GXPSVTo6kmR7Wehi5eDAnHVl/Leipdoe/Y4AD8NmExIfhYt8k4RmncaN2P19GiQ\n5+bDKd8WnPQLZXycNQ3GG7e+xIE2vTGras6LFYoaVjM7apWSOes/YVTCdvJ/WYdx6HCAWmtJL34x\nsrYuGkRDVxFiZSC47JgjumJu1RrN1i1gMIBG09wiNQsXYh5qKNVXDhU7Wr1L8pl/bDUttq5FVqko\nffwpTs96hp+iM2FNQr0blyrkM1cxfld8dtWoAcj0DWXuXe/xyM7F3HhoIx/9by5fjJnFzs4jOEWL\nBsmutJgZlbiDpzcvIMczgEIXT9rn1Ix6UskWSjWueJYVEZp3ukEpL8wKJcUuHhS5eOJiLCeguNIP\nUmHy0as0DvWWv4qcwdGgDpz2DaXExcN+vnt6HJ7lxexv17duR3Q9Os1otrCh61hGJWxHtXiRXRkY\njI5+jO4dLq5cbFVn8xtL9vHqg/2arCSmUAaCxqFQoJ8wEbevv0T91y6Mo0Y3t0TNwoVEEVWn6gpg\n9uSedbZbtCaeMXHbePDPH/DUF2Ps1ZuiD+dj7tadn6rNRCuUSJsQD7uZqE1I5SDYqaUPSen59s/p\n2UUc0OXYrxudNHw1+jHiwroyZ8sXPLfhE7qejOfbkY84pqKuA4tSRVTEKEqd3Yht0xuD2hn38mL6\npsXy3IZPHNp6lRcS26YP+9r1JSk0HKXFXGW1UYhXWVHlCsRhZVKEwUnDwdY9yPXwZ0yCdRV0LLAd\n7978Ah/+73ncDSXMnfYeKUHta8joVVpAaEEWsW16NyqvUXWOhIZzwr8VYRvWoc/JcQjdBejVMQBX\nF5XDKs3Xw5mnJnenVZBno3cgW+SmLYkplIGg0RjGW5WB84Z1160yuBRUDTF1XhNvT05WFVWyjrdX\nvELEqURKNa4sHPUo2/rcyOMeLajP6/DqA/1rPT/jlgj7ADRhQCvmLY2tNax0tzSUlMB2vLDuA8bH\nbUHKTOa9m57nlF/dKwQXjZJygwVZoeTvjoPs50tcPNjZeQQh+Znc8/dPrO8xHrNCxYCUGEbodjFC\ntwuTUkV8WAQx7foT077fefMMVWVh5Az+/dvb9MiIs/sJvhn5SK2KAECyJatLCjlvbZb6USjY2H0s\nM7cvYu1jr5M1fRbp2ZV+Go1aZd2oVoWqdZPrCjGuqiQenCA51KhuSoQyEDSanIjeaNy9MP2+hvxX\n5uHjdf2Vw2zyHETl5bh9+iGu8z/Bz2RkT4eBfDPqEXI9A8BiLdqiVMDdN1TGyDdk41JVE9TXaxwj\njqpT3Wz0yY/P2s1GAK7OKsr0ZvtnjUpFucFQZ38VNQti2/Qmpn1/vr9hBi2y0uifEsOAlH30TD9M\nz/TDzNixiLSA1sS078/edv04Ftyh3hm8Xu3CF/e9yTfzKqukbew+ts724bbNZkmhF6kMgO2dR/Lg\nn/9lXNwWHjt8m4OcMXVUlzsfi9cfsTuhS8qNvPpgv8uS70ooA0GjWfHncQa17sPoxO0Ujh2NZ9eO\nWAIDkQODMAcFIwcGYgkMsv4L0F6TBXEuJIqoOlMjO1BSZiQ5I5+E1FzSs4toFeSJetdOPOY+jVNq\nCjke/nwdOYO9HQbUuN8iW/MQvfbQpbEjV8zsq1KX2aj4rXcZMahSEb3w9d/k5JXV279viXWAy3Pz\nsfZtljmubcNxbRt+HjgFv6Jc+qftp39KDD3SDzN170qm7l3JOXdfYtr1JaZ9fw617F6rucr3WILD\n8ctr3mXezS/Y2wblZzHh8EZUFgttc9KwoCDZttksvFWl6awxuGiUlODBbmkIoxO30y0jjsOtetiv\n1zaZ9/Vwtg/oVX//yen59t9/chVZktPzL1v0kVAGggtiQ/dxdE8/TIu0BBSp8XW2k5VKZP8Am3II\nxBIUXPk5MMh2bP0se3heV+mxfTycycwtxWCykFtQzvf/3c2H6b/jsuJ/yEolpTNm8bhmuEOxeY2T\nssZs/kLtyFMjO2AwmknPLqZVkAf3jZPsduvHP9lpn/VDhdmoPS+ue5/xcVs4/kgy++Z9wc+nrEOI\nuQ5vq1qltJuh/IrzAMjz8K217TlPfzZ2H8fG7uNwMZTR88QhBqTG0C91P+PjtjA+bgv5rt7MnP4l\npc7u9vs8ywp5bv3HmBVKXp30H26LXUu/tP288vs8fu9zCxMObaRf6n57lFO5kzMZ/i3tfXhfQLbQ\niuispZuT2HB6HKMTtzMxbrODMqhAqcAha2nUgQxe+34fYF1RGUwWDFXMR+4uagy2zYbuLupGy3ah\nCGUgaDRTIzuwAvh6zAimDm+Dr74Y5ZlsVGeyUJw5g/JMtvVfdnbl5xPHcUqIq7df2c0Ni7bKqqIu\n5RGgBfXl+0/S5MgykYnbeeTPJbiUFWLs1oPijz7D1LM3ZdVCRBc+N5K9iVn29BAVXEg6ZB8PZzRq\nFbmF5eQWlqNRq+wmJLO55u7eTN8QnrvrPR7e+T0TD20gaPadJNrMRr6ezvh7uXCusNw+I/Z0UzPj\n5i72OsS+JVZlkG9bGdRHucaV6I4Die44EKXFjJSZzIzt39LhTCru+pJKZSDLPLVpAdriXJYOuYfD\nrXqQGNqFF/74gIEpMfRKt353UnAn2pw9gYtJj4tJj86W9mJU71BuHtwWZ2cnElNzOVtQMyS1Niqi\ns+4bGw43SGRELaT/0Wh8SvLJd698Picl3NC/JZv2ZtQIOQUcFK7F5hh4anL3ZkmDLpSBoNFUD32U\nccccFISZ81RBKy5GmXMGpV1hZNl+nnFQHk4H9qOoZTCqisXfH0tg5arCEhgE7Vrh7O7joExkb58r\nerXxXF93lHNepEvaP5hdXSl+fR5ljz4GTk6kZxfhrFagN1bOuvcmZjGgSzAh/u72AWPCwJY898Vf\nDk7GCQNasWB1HBaLTItA68B56kwJSqWCBydI7I6z5uipHgpZgcFU62mMThoWjp5JfFiEg9no19vn\n8NbjI+1x8RaLTAutu/17wFrKssDVC5OqcYrcolRxpEVnzngF0uFMKuU234OTSsHNsesYkLqPQy27\nsarfHQCYnNS8d9NcHtmxGBdjOX/0mMCx4A788tnkyufo14/FL0aSnl3E2/+NRalSEOzv1mBl4IBC\nwY4+N3Lfhi8ZnRDF6v532C95e7iwaW9GgxzAFb+n5tqUJpSB4PLh4YHFwwNL23b1t7NYUOTmVq4q\n7AqjmvI4dRKnI4624uoJMmRn58qVhdbRLGVdadh+agPB+TIWFzEYcPv8UyI++QCFXg8TJ5L/xntY\nWrayN1mwOs5BEYC1YtaALsEOA8ZjH+6oMdhUjU/Pq5YZtWpYrK+HM706BqBRqxqVObPCbPTCH5XR\nRq+mz+WkXxjdO/gRn3LO/r3OaiV6owWf0jxyPLV19umkBDdXNYUltZd5dDVYfRIVZrPWp49y344l\n5Lt689GEZxzSUphUahaOnmk/9inJx6lKLqMTrTtzcE2CQ8ruqj4PV2clZfranesVBYGq0u2VJ9Bv\n+Y5xcZv4pd9tDQ5ZbRPswfEsawTS5TQJ1YZQBoIrD6USWavFrNVijuhaf9uyMttqIxvf8kKKjh6v\nVXk4HT6Ewlh/LVmLr2+lktDW7d+Qff0uarXhFP03ns89iVOyDnNQMMXz3sf7oXuxnK2ZPqIhWKrY\n6xVYnY4FJQ1LjZ1XrKeT2oeZt0TYZ8kNJdM3hLnT3rWbjT7+8TlrtBEjHNrJMnjIRjz0pRwNrmki\nqlj9mCygVqmAOpSBsRyj0gmTSo2rvpTn132I2mLi4wlPk+dR/+Yu/yob1Eo0bmws8EAurDvapzZF\n4OWm5tGbu7BgdRw/R6U4bDZs0TGMXdJQxiRso0f6Yf5p3RNntcKukGtjwsCWdGntZ1/hDe0WXGfb\ny0GDlIEkSUrgbeABwBPYCDxuq2pWW/tI4B0gAsgEvtHpdB9cEokFgqq4umJp1RpLq9ag9aS8rnQU\nFguK/LwqJqpqPo0qysNJl1TvV8pqtU1ZBNWvPAKDwNU6i03PLuK7ZXuYuuU7RhzYiKxQUPbQI5S8\n/B9kL+9alUtF+GpRabndbDOwa834e7nKskCmciWgVIC3u3ONlcGcSd1YskFXY6D6YPlBSspr2ofU\nTrb4G04AACAASURBVEqMdYSgVpiN4lp25cnNn/Pchk/olhHPN6MqN6kZTBZ8C6yDcYl3QI0+qq5+\n6hs8XQ1l1lWBLDN720JCC7JY1fd2DrbpVec9FfhVUQbJIR0vaLNZYamRj38+ZN+Z/PGKQ3b/yL1j\nO7K/xzjGJGxj/OFNHG7Ts8aqrra0FHO/3GNfmSzZoGvWnEUNXRm8DtwH3AucA74CVgHDqzeUJKk9\nsBaYB0wFegP/lSSpWKfTfXUphBYIGo1Sieznj9nPH3N45/rb6vUoz+agzM6qR3lk45SYgOLggXq7\nsnh5Y9Fq0aYco0+V8+X3T0c/fiLKjAwsgQbwd69xb4UpqOoO1j1x2Twy0XG7mUqlxFzLYO2kUtKp\nlU8Nx2VaViEv39/HweEM1KoIAFzUKlrUkfiugr86DSFV244X/viAcfFb6JSVzPs3Wc1GYPUXAGQ5\ne9fZx/lwMZZRrnZhdEIUI5P+JClEYtmQexp0r39xZcK4pJBwOrfxIeVkYZ37LEL8XTHYBvOqCqpq\n0FTVoX7Z5qPMfPRW0rZ8yYCUGJ4YrGX+XzlcTZxXGdgK3j8JPKHT6aJs56YBaZIkDdTpdNHVbhkP\nlOp0urdtx8clSZoKjMOqRASCKxtnZywtwrC0CKu/nSyjKCyo4gCvNEsVpqaTlZiKdCoJ15Saee1d\nf/gO1x++qzyhUuFnj6QKJFvjRXSugjx3Hwa7+ZLv7sM5d1/y3GuGZVbdADclsj0bojMoKjVgMFnY\nm5hNiwA3Tp0ttbf/ZWcaNw1qWyNzal2J2YrKjBSV1W9ig/9v787DmyrTPo5/ky5ps7RNoSAIDCDw\nCA6IyIioiOLoiOOKooyK67iMKCCLgKIiyF5UdMAdZXOXl5FRFAXFUUdw39BnUEE2BaQtTdItTfL+\ncdKSpmmblLZJ4f5cFxf09CT9GXty5zzrgWajGz9YzNmfv36g2aj7wMqRRHkR8kfL6i3BVuLh5nVP\n4LZYmXvOmIgLzEUS2kyk23Rj154iHNbUGu9Eft1XzKKJg1j3+fYq+0sDlTuhhReSfse0IW3sbSRP\nHEv/z9ew+cTzWf2xsePb6X3a8vhrRv9W6GivSJMXG2OJ62hE80r2BuxA5Yb2WutflFJbgQFAeDHY\nC2QHC8aLGE1FpwL/bIC8QiQOk4lAZha+zCx8Xavu0DVy9jucU7iS7juCO4T1O5l/9riI0hQLVx6b\nwRFlrirFw5K3F3buInmzxvT1l3QAOkT4kQD+RfYqw27/kGznwj1+8m3ZdGyxmwcHHM3iL7y8u8OH\n35xEu1aOKsUAjE5nqLrI3o3n9eDJVcbeyLUt0lbbap7e5FReOP82vmzbo0qz0S5nG4CIxawuFTOd\nHcVGE2BaeSkzz76DPTEsWZFTdODOQB/RDXdRGVn2VJx2C/s9pRFH+3y3ZV+1QhC6P0FooajoVC69\n5FLsU+8mfeliht52O2f27cCL637k6x/zqhSeikIcaeRQaOd/Y65FFC6aYlDx8Whn2PFdQPsI578K\nLAKWA0uBJODFkDsFIQ5pyZ9uZO6S2/nD3l/It2ay8OzbuPLpe7ghdCP3sMfk5DjI2+sy7jY8bu6e\ntpJMTwFOTz7ZRQVc0TOjsnnKtGcPSbt/I2nrfzEFAnQCOlU80VvGX6OBkSYzRY4sUo5sw960TH4o\nSyPf6iTflkV+8C7jpcd38Ht6FsWp6Qw5rTO5I05m3IIPa12Euq5V7wce14b1nMHokGYjX7CNPt/m\nxJJiwp5uoU1La5W1/yMxm6DXUS357JsD+yS8cezZfKxOijzFN4JkM+QE70x2OI/EnW580i5wl9Gv\nR2sG9+vAgpXfUugupdRrfNo/vU9b5r/ydbXn2rmnqHKm8KA+7RnUp+pbYCAjk5KLLiF9+RJS3lvH\ni+42EecXJKJoioEV8GutwwcklwJpEc7PAjoCs4CXgJ7AfKXUFK31lPpHFSKxmQr3Y7t/CmmLF+EM\nBHiz55ksPuUqBpzeI/rRRyYTAbuDv159Jk+uMu4qbjivB54eEUaalJdj2rePqbNXke3JJ8tTgLMo\nn6Hd7ZX9G2l7dmPevo12bhe1NXqVJqeSt8hJectWTDDZyLc5ybM5cTmc9B3Yi+e/dZFnc7Lfmokv\nKRlz8O6gRWZatbH5K9/fwp+6t2ZDodFsdN36Zzn3qzcAo5lo6OldWLZmc41NNGao3HbSH4Ajc6x8\nV3bg3Of6D8NsNuH3RVcNyv2Qud/YgSzSekQdWjtYNPmsanthvPv5rmrn7issYeqzn+B0pNXYhFMy\n/BrSly8hfemzcNGkyuOWlCRKvT42bNqNNS3JmLAWQaOve1WDaIpBMWBWSpm11qGNZBbAE+H8OYBX\na31X8Ouvgv0Ojyql5mut82v7YTk5TdM+1lgkf3zFJX8gAK++CiNHwq+/QvfuTOo1nG+P7AHAWxt2\n0L9XO6Y/sxGAu649geNUq2pPE5r93IEOzh1YfTx7NW2cbGnVmdAdA66ed0H18zwe+O03+O03fv5M\ns27VRrI8+WS48nB6CnB6jL9b/fwd7cP3FngLQtdALcvKJrVdW7Zi42dfepU7jXyrk/12JyP+fAJF\nJV6++TmPx8+4ka/+0Iuuv23m16w21Zpeeh7VAv1LfmUbfHiX7or1W7i674Emof3pmVBHITCbjD8V\nzfoVfQY6ZKVSs9nELUN7k51hfKbNyXGQV1jC06/VvLwKGAVqX2EJC1Z+y6LJERbEO+s06N0by5uv\nM+KBh7BYjLfZ97840Ljy7ue7GHPFnwCq/Mzrz/8jTqcNc5Lx4cHptEX8nQ5/TEOIphhsD/7dhqpN\nRW2p3nQE0A9YEXZsA5CK0QxaazForjtVQfPeaQskf32Yt2/DPnEslrffImCxUDThLopuHc2mhz4M\nacYIcP+iDZVvdvcv2lBtkbuGzF7xPNU7IltBRisc3Xpxwd+M2bg3zHm3yoY3poAfR7GLbE8+Tk8+\n7fweLu+ZUW3Cn3/bdjoW7qdjTSGevZXpFgt7LZnkWbOCHeDZ/OPb/2NLwEa+1egQL7A5uf38k1m6\nfkvET+IVft8RMjKnlrss1T6LLIeFywZ1YedeN4+8+g0pJUXYS43PrRXLVtvTk0kymRnz4Hpuu7gn\nHq+fecs/q3WmsImqLVP7XaW8t3Erz642lsS+7eKeuIrKeOTVbzgrpz83+b4k9dlnuOb28UDVYgBE\n3PO6tLScH3fsP7ChzVMfR+wzCH/M5OtPrDl4lKIpBl8BbmAg8ByAUqojRlPQ+xHO3wHV1iXoCfiA\nn+qZU4jEUl5O+pOPYZt9P6aiIspOORX33AfxHWV8mr8h2BkLcOGpnVixvvpuXw1lyMADzz9kYGXv\nQbWOyGsGq2q7s40OroNT7vPjD0DAZKbQmkmhNZOtOR2ha0uKL468zMj+3wtY/dqn/PLlZpyefJxF\n+Tg9BVzY1VpZPDJ2/YZzz8+k+GtY3wLgqRu4wWLnEruTfKtxl5HVpT2fF6ZUfm0u/LXydKfdgqfE\nS5sWVn7ZXXW464Qr+lT+O8tu4bFxp+H5ehM8DMUpaezMaU+GNQV3sbfyjX/W8s8pKat9+RM4UAjM\nJuPuoKzcX22To4pRXGu7DeCq954hbdliXj9lKEvX/lzluSwpibeSb53FQGtdppRaCOQqpfZhjBZa\nALyrtd4YbALKBvK01l5gPrBKKXUXRvE4BpgHLNBa12+KpRAJJPnLz7GPHUXKN1/hz87GNWsepZdd\nXuUTa78eR9Av2M4/fuFHVR5f0Q4c7U5ndTm3fyfO7d+pzvMi7c5WsRR3gbuURW98z/+2FZCSbKbM\n68dkgkF9at7MJrNlFsOu+zPXzao6gevPYZOrrp+1FnuxC6engKxgwcj25HNRNyvbvv6R5L17cBTu\nI8tTQId9wYaIH4xhjJE889TfK0dTvburnIJgp3ieLZvkT2xVJvxdN2sdPbd9wwzgf0d0xZScQmFR\n1WGykQrB0R2y0NsLInaWOx1ptU6OAyi2WPmg+6mc+fUavnvqFejUp8r37ekHlp4IXQbkskFdKPSU\n1dlnEP6YhmAK1DU0AFBKJWF0CF8NpACrMeYd5CmlBgLrgNO11u8Hzz8fmAwcDfwGLAFmRuiEDheQ\nZor4kfy1M7ldWGfdT/pTj2Py+ym57HLcU6YTaFHz/sfbdruY+uwnlZ9CW2SkVd72h97qn3rckRF3\nOjsY4c1EM5Z+VlkMUpPNNe7HMH7hR5VvdqF5K4SvkPrBN7sq70zMZsi0HtjaEagyaS5Ui4zqb6rJ\n5V6yigt44OKjMO/ZQ8n2nbz7xmcM+/C5ynN2Z7SiRXEByd6aN9IBY8LfriQ77fIPNM8sHXg1v6dl\nHujjsGVRmJ5RbUZyarKZzm0zquxz4LRb6NYhi8H9OrB6wzbAWEIiUjMRwJ09Ahx/zQX896h+zLjg\nQEdyi4yaO5/rKyfHcdCrMUY1YyP4Jj4++Cf8e+sxho+GHnsNeO1gwwmRKFJXv4590jiSdu2kvPNR\nuHPnV26CXptHXv2mshCYTU07OiR8DHtDjVKJtF3j74VlvP/FTvx+YzmMGUs/Y9bN/cmyWziqbQY/\n7Sqs9jyRPl2XJ6fwuyOHNbRm0Nl9efK179jQ7yjM3jIu3fgKH3Q7idnn3gGBALZSD86iAv4Q8HBL\n/5ZhK+Aas8ftW3dUef7h6xdX+5k+k5kCa0iBsGbhcmRj69ye7MJkCmxZ5FudeJNacdP5JzN18cbK\n2di78z3VimVokfX26k2/bz8l251Hnj2bK8/qWm04aqKQheqEqIX5113YJ43H8sYqAikpeMbcQdHo\ncZAWaVR17ZyOtCqfBkNv768//4/4Suue5Rutf/93S5V+hHP7d4p6d7ZLTutc2d/Rq0s2f59tfLK/\n4bwelU1fdSkr9/Piuh+56fxjGDGkZ5U7ifBiAsbm8V9s/r3y62VrNtOn24ERV6+ccDH5tizWHhPc\nc9tkwpNmx5NmpzgjjdIhkSdmXTdrHcnlXjrv3YLPbOa+czqw9cvNfPPRdzg9BfSye3H9vAOnJ4/2\neTvosiekbf9jYzmFUP6FdialOMi3ZbHD2Y4lA4bX+jqUDL8Gx/jRLHT+RNHYS2o9tz57UjSkqJqJ\nmpA0E8WR5A/h85H2zJPYZkzD7HZRduJJuHPn4+sWW1NOtEsLNPRrH940E2mRtJqENhOFMpvgqQmD\nIr5pJVlSmLv0EzZtyau8E+rXo3W1JS/AeNML3X8hNdnMrJv7M+afH1Y5r1+P1jUWjyEDO7H+i1/Z\n7ymlPDjMtHvHLMYPq9o2D3X/P6h8rQIB0r0lPHalqpyn8cLyD8gqyg+OriqgAx6S9u4hs2g/SQE/\nm1sfhfOj9QQcGRFfF5PbRXZPRcDpJO+Tr2vdAja02TD0vz30+WrSZM1EQhxOkr75Gsf4UaR8/hn+\nzCxcDzxCyeXDjQbxGMVro5LGFL65EUB2RhpjLu1d7Q0RIn/ivf3SY6u8QVd8v6afFV4MKjrNQ4ve\n91sj72Nc1/IOlaOxTCYGn9kDX+dO+Dob2f/9Q1a1TuSObexs27mfEe88ylnfvkPZFZey/4UVvLhu\nS7Xms4DdQemQoaQvfYbUd9+h7M9/iZgxktAiuGHT7kZvYpJiIEQFjwfb3JmkP74Ak89HyZChuKfO\nJNCq+gSxRPbv/1Ydxho63DQaoX0LA49rw8r3jee74bwetT4ufDvOru0yGNSnfbU3tb5Ht+SWC3tV\nabIK3REtNdnMMZ2yqzSjDT6xfeWib4NPbNg3xHP7d+La83tFvDO78bwe1bYYvefq4BS8CYMo+cf1\npK1cQeY1l5N02b0Rn7/k6mtJX/oMaUue4ceeJ9V4lxI+Qii8QC5bs1mKgRCNLfWdt7BPGEvS9m34\n/tAR15wH8Z5+Rrxj1Uv4nIZohp2GCr+bifbxFf0MFWp68/r0h9+rHbtsUJfKyVZl5X5SU5KqNIv0\n634EGzftrfx3he4dsyrvCLp3jLy38sF0nPfrcQTb9rgiF6KkJFwLnsRUVIRlzZvcZknHP2QigaTk\nKm/s5b164z32OFLXvMmy7uvYZzIKQPhdSvgd12WDujTpukZSDMRhzbz7N2yTJ5L2rxUEkpMpGjkG\nz5g7wGqNd7RDRjRvall2C13aZbJvU+Tx+zU19UTqI9iw6bcq6zr163FE1E11kfoXhp7WlaGn1bA0\nSEoKhU8uJvOKodjfep2xmRm4Hn60WpNiyVXX4hg7kgFfrOHHPhdHlSXLbuHKs7pWWxm1scTeCCrE\nocDvJ+3Zp3Ge/CfS/rUCb98TyH/nP3gmT2n2hSC0WSjWJqJQ23a7GL/wI8Yv/Ihtu+vu3A5vRqp4\n88qyW+h79IEdzkL/HWpwvw60yEijRUYag/vVtIC3YcayT7lu1jqum7WOGcs+rfK9J1dtwh8wZgmH\n363UpaLo7CssqSwKdUpPp3DJ83iP70vaS89jnzSu2tKuJRddgt/u4Nwf1tLSnlI516Aug/q0Z9HE\nQSyaOIjWTis3577Hzbnv8d2WfXU+NlZyZyAOO0nfb8IxbhQpn2zA78jANfsBSq6+rl4dxIko2hnJ\ndYl1Xf3QWdfhbrkw8pIWoVZv2Fb581Zv2FalySS8qWfKM59Ufu/HHdXnMDSmSHceAbuD/c+/StaF\nfyX9macIODKMDxYV7HZKL76U9MVP81A3N2VnDY7550aaQd6QDo3ffiGiUVyMbfp9OM84hZRPNlBy\n/kXkf/QpJdf+/ZApBIeqin6MubecVOfM3RvO61G5amldnd7hbru4Z+XdSU2f3Gu68whkOSl4aSXl\nR3XB+vADpM+fV+VxJVddA0DakmdiytRU5M5AHBZS3luHY/xokn7Ziq9de9yz51F2ZviUIhGqqdfV\nj2W9nS7tMirvCLq0ywCqDmHNHXFyvSZtHexQ4ECrVux/5TWyzvsL9un3EbDbKbn+JgDKex6L97g+\npL6zBvPOHXVvqxqmsf9/yKSzBiSTtuIrUn7T3r3Y75lE2qsvETCbKb5pBJ7xk8Buj1PKyA7F176p\nhU/aijThrSax5I/UTBTO/PNPZJ1/Nkl7dlP48KOUDrsCgLTlS3DcfiuecRMpuuPOqPNFkV8mnQkR\nUSBA2nNLsd03GXNBAd7ex+Ge9zDlPY+NdzLRzNXWN1LB3/ko9r/8L7IuHIxj9AgCNhtl511IyQVD\nsN09ibTlSygacwckJ85bsDSUikNO0ub/kXnhOThuvxW85binz6Zg9TopBIeY77bsqzK6xpp2YKmH\n0H/Hi697D/a/sIKA1UbGzdeTunaN0ZF8yaUk/bqL1HfWxDtiFVIMxKGjpATrnBk4Tz+J1P9+SOnZ\nfyX/g40U3/CPWteEEc1Txeiaik1mQndKq23XtKZUftzxFC5/CZKTybj2SlI++oDiq64DIG1pYnUk\nSzEQh4SUD/8Dxx6LLXcW/hYt2f/scxQueT7mTjohGpq3/8nsf2YZ+HxkXHkZJm8Z3uP7krr2bcw7\nttf9BE1EioFo1kx5+7CPuoWsi/4KmzdTdMPN5H+wkbJzzo13NNHIbru4J6nJZlKTzdx2cc8qS0U0\n9PpFB8s76EwKH1uEqchD5mUX4e3bD5PfT9qy6vsrxEu0O52ZgekYO505gDeBEVrrPTWcfyTG9pdn\nAcXAK8BYrXXte8XJaKK4alb5AwEsL7+A/d47Me/bh/ePvUhZ9BR7Ox4d72T10qxe+wiiyR/v9fpr\n01Svv+WF5WSM/Ad+uwOz24XviDbkff7dQXckN8RoomjvDO4DhgNXAgOAdhhv8NUopVKBd4AsoD9w\nKXAuMOdgwwoBkPTzj2RecgEZt96EqbgY95TpFKx5D/70p3hHE7WoWL10w6bdNS5ZfagrHXYFrpm5\nmN1G4Un67VdS334rzqkMdZaj4Ib3IzH2PF4XPDYM2KKUOlFr/XHYQ64AWgP9tNaFwfPvAf7RoMnF\n4aesDOuC+VgfmIOptJTSP5+Fe9Y8/B3+EO9kQkSt5PobMXnc2O+fAsDme+dy31fpQGybEDW0aO5N\negN2YH3FAa31L0qprRh3CeHF4Czg7YpCEDx/MZA4jWOi2Un++L84xo8iWf+Ar1Vr3DPmUHbehWA6\n6Ltj0URimWF8qCseOQazy4V1/jz6bP2CVvt3syezdVwzRVMMKoZj7Aw7vguI1EvTDVirlJqK0awU\nAFYAk7XWpfUNKg5PpoJ8bNOmkL70GQImE8XXXI/nrnsJZEZeu14krkg7pB3OPHfew9r133Pel2/Q\nYd/2ZlEMrIBfa+0LO14KRNoVPAP4O/AGcAlwJLAAyAGuqXdScXgJBLCsfBX75ImY9+6hvHsPXLnz\nKf9Tv3gnE6JhmEw8cfoNvN77r+x0to13mrpHEymlhgAvAylaa3/I8Q+AT7TWt4edvwlIAbpprQPB\nYxcDLwEttdb5tfy4hFooScTJli1wyy3w5puQlgb33gtjx0JKSryTCZGommRtoopZEW2o2lTUlupN\nRwSPFVcUgqBNGGE7ArUVg0N+eF0ii3t+r5f0xxZgy52JqbiYsoGn45rzIP5OnaGgBKh9ZHLc8x+E\n5pwdJH+85eTUvqx3NKIZWvoV4AYGVhxQSnXEeGN/P8L5/wF6K6VC5//3BMqBrfXMKQ5xyZ99gvPM\ngdin3UPAZqNw4ZPsf2mlUQiEEI2uzjsDrXWZUmohkKuU2gfsxegDeFdrvTE49DQbyNNae4HHgFuB\nJcFO5PYYcwwW19FEJA5DpsL92GZMJe2ZpzAFAhRfcRWee6YScGbHO5oQh5VoJ51NBpYDS4G1wBZg\naPB7J2GMLOoPEJyVfCpGgfgMWIbR53BLg6UWzV8gQOqqf+E85QTSFz2Jr0tXCv61GveD/5RCIEQc\nyOY2DehQaHdsivzmHduxTxqH5a3VBFJTKRo9jqLbbgfLwS1P0Jxf/+acHSR/vMnmNqJ5KS8n/enH\nsc28H1ORh7KTB+Ce+xC+Ll3jnUyIw54UA9Ekkr/6AvvYUaR8/SV+pxPXrFxKL7tcZhALkSCkGIjG\n5XZjm30/6U8+hsnvp+TSv+GeMp1Ay5bxTiaECCHFQDSa1LdWY584lqSdOyjv1Bn33IfwnnpavGMJ\nISKQYiAanPnXXdjvvAPL668RSEnBM2Y8RaPHG7OJhRAJSYqBaDg+H2nPPo1t+n2Y3S68J5yIa97D\n+FTz3HBGiMOJFAPRIJK+/QbH+FGkfPYp/swsXPMepuSKq8AsO6sK0RxIMRAHx+PBljuL9Mf+icnn\no2TIJbinziLQqlW8kwkhYiDFQNRbyrq3cdwxhqRtv+Dr0BHXnHl4B50Z71hCiHqQYiBiZtq9G/vd\nE0hbuYJAUhJFt92OZ+wEsFrjHU0IUU9SDET0/H7Sli3GNu1ezPsL8B7fF1fuw/iO+WO8kwkhDpIU\nAxGVpB++xzFuFCkbP8bvyMA1ax4lV18HSUl1P1gIkfCkGIjaFRdjfWgu1n/Ox+T1UnruBbhnzMF/\nRJt4JxNCNCApBqJGKe+/h338aJK3/IzvyHa4Z82j7C+D4x1LCNEIpBiIaky//4793jtJe/kFAmYz\nRTeNwDPhLrDb4x1NCNFIpBiIAwIBLM8vwz7lLsz5+XiPPQ73vPmU9+od72RCiEYWVTFQSpmB6cDV\ngAN4ExgR3NWsrsf+G7BqrQcdTFDRuJJ+3AxDx5Cxfj0Bqw33tJkUX38TJMvnBSEOB9GuFXAfMBy4\nEhgAtANeqetBSqmbgHPqnU40vtJSrHNn4jytP6xfT+nZ55D34ScU3zRCCoEQh5E6r/bghvcjgVu1\n1uuCx4YBW5RSJ2qtP67hcV0w7iY+asC8ogGlfPQB9nGjSP5xM74j2pC0cAGFJ58hG84IcRiK5s6g\nN2AH1lcc0Fr/AmzFuEuoJtistBiYBXx/0ClFgzLl52EfPYKsC88h6acfKb7+RvI//AQuukgKgRCH\nqWiKQbvg3zvDju8C2tfwmDsBv9Y6t77BRCMIBLC8/ALZJ/cl/bmllB/Tk4LVa3HPzCXgyIh3OiFE\nHEXTKGzFeGP3hR0vBartVqKUOh64Heh78PFEQzH//BOOO8aQ+v67BNLTcd8zjeKbboGUlHhHE0Ik\ngGiKQTFgVkqZtdb+kOMWwBN6olLKAiwBJmutt9QnUE6Ooz4PSxgJl7+sDHJzYdo0KCmBwYMxLVyI\nvWNHIs0aSLj8MWrO+ZtzdpD8zV00xWB78O82VG0qakv1pqN+wNHAbKXUnOAxC0YxKQR6aK131PbD\n9u51RREpMeXkOBIqf/KGj3GMH0XyD9/jz2mF++FHKb1giNEvECFnouWPVXPO35yzg+SPt4YoZNEU\ng68ANzAQeA5AKdUR6Ai8H3buBqBr2LGZQAfgcox+BtHITPsLsE2bQvqSRQAUX3Udnsn3EshyxjmZ\nECJR1VkMtNZlSqmFQK5Sah+wF1gAvKu13hgcepoN5GmtS4GfQx8fvCMorm+zkYhBIIDlXyuw3zUB\n8949lKujceU+THm/E+OdTAiR4KKdVTQ5eO5SIAVYDdwa/N5JwDrgdKrfKYgmYv5lK/aJY7GsfZuA\nxYLnznsoumUkpKbGO5oQohmIqhgERxKND/4J/956oMZF7bXWN9Q7nahbeTnpjy3ANncGpuJiygac\nhmvug/g7HxXvZEKIZkTWG2jGkj//FMfYUSR/9w3+Fi1w5c6n9JLLZOKYECJmUgyaIZOrEOvMaaQ/\n/QSmQIDiy4fjuWcqgewW8Y4mhGimpBg0M6mvr8J+53iSft1FeZeuuHPn4z3plHjHEkI0c1IMmgnz\nzh3YJ43H8ubrBFJT8YyfRNHIMWCxxDuaEOIQIMUg0fl8pD/9ONaZ92P2uCk76RTccx/C17VbvJMJ\nIQ4hUgwSWPLXX2IfO4qUr77A73RSOGMhpcOukA5iIUSDk2KQiNxubHNmkP7EQkx+PyVDh+G+bwaB\nli3jnUwIcYiSYpBgUtesxj5xHEk7tuPr2AnX3IfwDjw93rGEEIc4KQYJwvzbr9jvmoBl1UoCUzXM\nWQAADUZJREFUycl4Ro+j6PbxkJ4e72hCiMOAFIN48/lIW7wI2/T7MLsK8f6pH655D+M7unu8kwkh\nDiNSDOIo6btvcYwbScpnn+LPyMQ19yFKhl8D5mg2oBNCiIYjxSAeioqwzZtN+qOPYCovp+TCIbin\nzSbQunW8kwkhDlNSDJpYyrp3cNwxhqRtW/G174B7zgOUnXFWvGMJIQ5zUgyaiGnPHuz3TCRtxSsE\nkpIoGjEKz7iJYLPFO5oQQkgxaHR+P2nLl2Cbeg/m/QV4+xyPK/dhfH/sGe9kQghRSYpBI0rSP+AY\nN4qUDf/Fb3fgmjmXkmv+Dkk1bv8ghBBxEVUxUEqZgenA1YADeBMYobXeU8P5lwETMfZD3gU8DczV\nWvsbInTCKynB+tBcrI88hMnrpfSv5+OeMQd/m7bxTiaEEBFFe2dwHzAcuBLIAx4FXgFODT9RKTUY\nWAaMxCgaxwFPBX/W9IOPnNhS/rMe+/jRJP/8E762R+KeNY+ys8+JdywhhKhVncUguOH9SOBWrfW6\n4LFhwBal1Ila64/DHnIT8LLW+tHg11uUUj2AazmEi4Fp3z4YdytZS5YQMJspuukWiibcRcDuiHc0\nIYSoUzR3Br0BO7C+4oDW+hel1FZgABBeDKYBnrBjAcBZ75SJLBDA8uJz2KfcBXl5eHv1xj1vPuXH\nHhfvZEIIEbVoikG74N87w47vAtqHn6y1/iz0a6VUBnAzsLo+ARNZ0k+bsY+/ndQP3idgtcEDD1Aw\n7BpIln55IUTzEs27lhXwa619YcdLgbTaHqiUSgdWBs+bVK+Eiai0FOsjD2J9KBdTWRmlfxmMe2Yu\nLY7rAXtd8U4nhBAxi6YYFANmpZQ5bDSQherNQZWUUi2AVcDRwJ+11tujCZSTk+Bt7P/5D9x4I/zw\nA7RpA488gmXIECzBDWcSPn8dJH/8NOfsIPmbu2iKQcWbeBuqNhW1pXrTEQBKqY7AGsAGDNBafxdt\noL0J+snalJ+Hbeo9pC9fQsBkouTav+O5614CGZnwuxswfpkSNX80JH/8NOfsIPnjrSEKWTTF4CvA\nDQwEnoPKN/uOwPvhJyulcoB3gTKgv9Z620GnjKdAAMurL2G/ZxLm33+nvPsxuObNp7zvCfFOJoQQ\nDabOYqC1LlNKLQRylVL7gL3AAuBdrfXG4NDTbCBPa+0FFga/HgSUKqUqluIM1DRJLVGZt/yMY8IY\nUt9bRyA9HffdUym+eQSkpMQ7mhBCNKhoh71MDp67FEjBGBl0a/B7JwHrgNOVUhuBiwATsDHk8Sag\nHEhtgMyNz+slfeHD2ObNxlRSQtnpZ+Ca/QD+jp3inUwIIRpFVMUgOJJofPBP+PfWA6GL7TTrcZXJ\nn2zAMW4Uyd9vwt8yB9f8hZReeDEEO4iFEOJQ1KzfuBuSaX8Btun3kbZ4EaZAgOLh1+C5+z4CWYfm\nXDkhhAglxSAQIHXVSux33kHSnt2Uq6NxzZ1P+Yn9451MCCGazGFdDMzbt2GfOBbL228RsFjwTLqb\nohGjILV5dG0IIURDOTyLQXk56U88im3OdExFRZQNGIh77oP4OneJdzIhhIiLw64YJH/xGfaxo0j5\n9mv8LVrgmvMgpUOHSQexEOKwdtgUA5PbhXXmNNKffgKT30/JsCtw33s/gRYt4h1NCCHi7rAoBqlv\n/Bv7pHEk/bqL8qO64M6dj/fkAfGOJYQQCeOQLgbmXTuxTxqPZfW/CaSm4hk3kaKRYyCt1sVWhRDi\nsHNoFgOfj/RFT2CdMQ2zx01Z/5Nx587H17VbvJMJIURCOuSKQfI3X2EfO5KUL7/An5WF68F/UvK3\nK8Fsjnc0IYRIWIdOMfB4sM2ZQfoTCzH5fJRcfCnuqTMJ5OTEO5kQQiS8Q6IYpL79JvYJY0nasR3f\nHzrimvMg3tPPiHcsIYRoNpp1MTDv/g3bXRNIe+3/CCQnUzRqLJ4xd0B6eryjCSFEs9I8i4HfT9ri\nRdjun4LZVYi37wm4cufj63FMvJMJIUSz1OyKQdL3m3CMHUnKpxvxZ2TimvMgJVddKx3EQghxEKIq\nBkopMzAduBpwAG8CI2rauUwp1Rd4CDgO2AHcr7VeelBJi4uxPTCH9AXzMZWXU3LBEDz3z8Lf+oiD\nelohhBAQ7cfp+4DhwJXAAKAd8EqkE5VSLTGKxacYxeAR4Gml1J/rGzLlvXVkn9oP6/x5+Nu0Zf9z\nL+N68lkpBEII0UDqvDMI7nE8ErhVa70ueGwYsEUpdaLW+uOwh9wAFGitRwe//p9Sqg8wDngnlnCm\nvXux3z2RtBUvE0hKouiWkXjGTwKbLZanEUIIUYdo7gx6A3ZgfcUBrfUvwFaMu4RwpwDvhx17Dzg5\n6lR+P2nLl5B98vGkrXgZ73F9yF+zHs+U+6UQCCFEI4imGLQL/r0z7PguoH0N50c616qUyq7rhyX9\nT5N54Tk4br8VvOW4Zsyh4I21+Hr2iiKqEEKI+oimA9kK+LXWvrDjpUCkFd+sQEmEc6nh/ErP9x/G\n0E9eJdlXTuk55+GeMQd/2yOjiCiEEOJgRHNnUAyYgyOKQlkATw3nWyKcSw3nV/rbxy9SkJ7J/sXP\nU/jscikEQgjRRKK5M9ge/LsNVZt/2lK9Oaji/DZhx9oCbq31/tp+0HljVgKw6qoLmu22Yzk5jnhH\nOCiSP36ac3aQ/M1dNMXgK8ANDASeA1BKdQQ6Ur2jGOAD4JqwY4OAD+v6QavmNd8iIIQQzZkpEAjU\neZJSaibGhLNrgb3AAqBIa31GcOhpNpCntfYqpVoBPwAvAvOBM4G5wF+01usj/gAhhBBxFe2ks8nA\ncmApsBbYAgwNfu8kjNFC/QGCs5LPxphw9jlwCzBcCoEQQiSuqO4MhBBCHNpkdTchhBBSDIQQQjTh\nEtYJsfLpQahH/suAiUBXjD6Vp4G5Wmt/0ySuliem/GGP/Tdg1VoPatyUtWaI9fU/EmMAw1kYc19e\nAcZqrcMnRDaJeuQfBMwEjgF+BZ7QWs9torg1Uko9Bpi11jfWck5CXbuhosyfUNduqGjyh50f9bXb\nlHcGcV35tAHEkn8wsAx4AuiJ8Ys1AZjUJEkjizp/KKXUTcA5jRstKrG8/qkYiyJmYQxsuBQ4F5jT\nJEkjiyX/UcAq4DXgjxi/O/cqpf7RNFEjU0pNBWp9E0rQaxeIOn8iXrtAdPnDzo/p2m2SO4N4rnza\nEOqR/ybgZa31o8GvtyilemAMzZ3eVLkr1CN/xeO6YOT9qMnCRs4Ra/4rgNZAP611YfD8e4C4vJnW\nI//ZGEO3K35XtgY/rf4FeJQmppTqhPHp+BjglzpOT6hrF2LOn1DXLsScv+IxMV+7TXVn0PQrnzas\nWPNPA6aGHQsAzkbKV5dY81c0aywGZgHfN37EWsWa/yzg7YpCEDx/sdb6xEbOWZNY8+8FspVSw5RS\nJqXUH4FTgU+aIGskJwHbMD4pb63j3ES7diG2/Il27UJs+et97TZVn0F9Vj79PMK5VqVUttY6r4Hz\n1SWm/Frrz0K/VkplADcDqxslXd1iff0B7sRYoDBXKfVkoyWLTqz5uwFrg7fVV2JczCuAyVrr0gjn\nN7ZY878KLOLA3J4k4MWQO4UmpbVeHsyCUqqu0xPt2o0pfwJeu7G+/lDPa7ep7gyabOXTRhJr/kpK\nqXRgZfC8eLU7xpRfKXU8cDtwVRNki0asr38G8HegM3AJMBq4DHi8MUPWItb8WRjLvcwC+mL8fzhL\nKTWlETM2lES7dustQa7dmBzMtdtUxaDJVj5tJLHmB0Ap1QJjxnZvjOU4ttd0biOLOr9SygIswfgU\nvaWJ8tUl1tffC+zDmPn+udZ6FcYFMlwpFY/b/VjzzwG8Wuu7tNZfaa2XYbS5T4xT/lgk2rVbLwl0\n7UbtYK/dpmomarKVTxtJrPkrFvNbA9iAAVrr7xozYB1iyd8POBqYrZSqGH1jwXgzKwR6aK13NGbY\nCGJ9/XcCxVrr0On1mwATxifu/EbIWJtY8/fDaNYKtQFIBTrQ9PljkWjXbswS7NqNxUFdu011ZxC6\n8ikQ1cqnp4Ydi2rl00YSU36lVA7wLkZbdf8E+GWKJf8GjPHVvYFjg3/+D6Pz8liM9t+mFuvvz3+A\n3kqppJBjPYFyouiAawSx5t8BhG/t1xPwAT81SsKGk2jXbkwS8NqNxUFdu01yZ6C1LlNKLQRylVL7\nOLDy6bta643hK59iDKMar5R6lAMrnw7DGFrX5OqRf2Hw60FAqVKqdfCpAtFM8opz/lLg59DHBz9V\nFMer2ager/9jwK3AkmAncnuMppfFWusm/1Rdj/zzgVVKqbswlo0/BpgHLNBau5s6f20S/dqtS6Jf\nu3VpyGu3KSedNfeVT6PKr5RKAy7CGEq4MXh8F8Ys0qZuXgkV9eufoGL9/TkV4yL5DGMS0csYv0fx\nEkv+1cAQ4AKMu4oHMArc2KaNHFH4ypbN4doNVWP+BL52Q9X6+h8MWbVUCCGELFQnhBBCioEQQgik\nGAghhECKgRBCCKQYCCGEQIqBEEIIpBgIIYRAioEQQgikGAghhAD+H/tpxp/gTOfbAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee4a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(metropolis_samples[:,0], metropolis_samples[:,4], \".\")\n",
    "plt.plot(unique_samples_met[0:50,0], unique_samples_met[0:50,4], 'r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the plots above, we can see that both samples converged. The NUTS sampler converges to the center of the points more quickly and stayed around the convergence more quickly. This would be helpful when models get more complicated as this is a simple example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS with dual averaging makes it possible for Bayesian data analysts to obtain the efficiency of HMC without spending time and effort hand-tuning HMC's parameters, making it possible to efficiently perform Bayesian posterior inference on a large class of complex, high-dimesional models with minimal human intervention.  \n",
    "\n",
    "We implemented the NUTS sampler in Python and were able to optimize our implementation of the NUTS sampler by removing the need to re-calculate the gradient and the log likelihood. We ran the NUTS sampler on various examples and compared them to another MCMC algorithm, Metropolis Hastings. \n",
    "\n",
    "The NUTS sampler appears to be overly complicated for simple examples, such as the ones we chose to show. However, it can very useful and efficient for higher order dimensional Bayesian MCMC problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. M. Hofmman and A. Gelman, \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\", Journal of Machine Learning Research 15 (2014) 1351-1381\n",
    "\n",
    "2. A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari and D. Rubin, \"Bayesian Data Analysis\", Third Edition, CRC Press, 2014\n",
    "\n",
    "3. S. Chibb and E. Greenberg, \"Understanding the Metropolis-Hastings Algorithm\", The American Statistician, November 1995, Vol. 49, No. 4\n",
    "\n",
    "4. C. Robert and G. Casella, \"A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data\", Statistical Science, 2011, Vol. 26, No. 1, 102-115\n",
    "\n",
    "5. C. Andrieu and J. Thoms, \"A tutorial on adaptive MCMC\", Stat Comput (2008) 18: 343–373"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
